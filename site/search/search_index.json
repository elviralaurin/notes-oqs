{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>These notes are my attempt to understand quantum mechanics and open quantum systems. They are written in an effort to be easily understood rather than condensed into a clean set of definitions and proofs. As a result, they tend to get a bit wordy, so be warned.</p> <p>All mistakes are my own. If you spot an error or disagree with anything, please leave a comment or a pull request in the GitHub repository for this project.</p>"},{"location":"abstract-vector-spaces/","title":"Abstract vector spaces","text":"<p>Why is this relevant?</p> <p>The language of quantum mechanics is linear algebra, and everything we do is done in special vector spaces called Hilbert spaces. Hilbert spaces can be infinite-dimensional (don't worry, we'll get there), and the \"vectors\" we work with aren't really vectors in the sense you'd learn about in a basic linear algebra course. Instead, they are functions.</p>"},{"location":"abstract-vector-spaces/#what-we-mean-by-abstract","title":"What we mean by \"abstract\"","text":"<p>Abstract linear algebra is what happens when we do normal linear algebra, but we stop caring about the details of what a vector \"actually\" is. Instead, we just formulate a set of definitions (the same ones as in normal linear algebra, in fact), and base all our logic on those definitions - we make no additional assumptions about the underlying mathematical objects. In other words, we take the findings of normal linear algebra and abstract them so that we are able to apply them to a wider range of objects and topics.</p> <p>For example, consider the vector space \\(R^3\\). It contains any vector in any direction you can literally think of. You can add any two (or more) vectors, stretch or shrink them by multiplying them with scalars, all to your heart's content. Also, you can condense the information of a vector space into just a set of basis vectors, which you can then use as a kind of common language to express any vector in the space. The basis vectors can be arbitrarily chosen, as long as there is at least one representative of each direction of the space, which in math is called linear independence.</p> <p>If we play around with vectors like this for a while, we will start noticing certain properties that seem to be universal for all vector spaces. Formally, there are eight such properties, and we'll use them as axioms for defining what a vector space actually is.</p>"},{"location":"abstract-vector-spaces/#what-makes-a-vector-space","title":"What makes a vector space?","text":"<p>Vector spaces</p> <p>A vector space is a set \\(V\\), that has an addition operation (\\(+\\)) and a scalar multiplication operation (\\(\\cdot\\)), for which the following eight axioms hold (assume that all symbols with an arrow \u2014 e.g. \\(\\vec{v}\\) \u2014 are elements of the set \\(V\\), and all (lowercase) symbols without an arrow \u2014 e.g. \\(a\\) \u2014 are scalars)[1]:</p> <ol> <li>When adding elements, the order of the arguments doesn't matter: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\)</li> <li>When adding more than two arguments, it doesn't matter which two arguments we add together first: \\((\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\\)</li> <li>There is a special element \\(\\vec{0} \\in V\\) that, when added to any other element \\(\\vec{v}\\), leaves \\(\\vec{v}\\) unchanged: \\(\\vec{v} + \\vec{0} = \\vec{v}\\)</li> <li>Every single element \\(\\vec{v}\\) has an opposite \\(-\\vec{v} \\in V\\), so that when you add them together, you get \\(\\vec{0}\\): \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\)</li> <li>When multiplying the scalar \\(1\\) with any element \\(\\vec{v}\\), that element remains unchanged: \\(1 \\cdot \\vec{v} = \\vec{v}\\)</li> <li>There is a distributive rule for sums of scalars like so: \\((a + b)\\cdot\\vec{v} = a \\cdot \\vec{v} + b \\cdot \\vec{v}\\)</li> <li>When multiplying multiple scalars with elements of \\(V\\), the order doesn't matter: \\((a \\cdot b) \\cdot \\vec{v} = a \\cdot (b \\cdot \\vec{v})\\)</li> <li>There is a distributive rule for sums of elements like so: \\(a \\cdot (\\vec{v} + \\vec{u}) = a \\cdot \\vec{v} + a \\cdot \\vec{u}\\)</li> </ol> <p>I'm playing a little fast and loose with the term \"scalar\" here. At this point, just assume that a scalar is a real number.</p> <p>All of these properties probably feel quite trivial. There are some interesting things to note though. First, the way we've written it, whenever we use scalar multiplication we always write the scalar on the left and the element of \\(V\\) on the right. Second \u2014 and most important \u2014 we're really not making any assumptions about the elements of \\(V\\) at all. What this list of properties is really telling us are expectations about how the operations of addition and scalar multiplication behave. So what makes a vector space a vector space really isn't the vectors themselves; it's what we can do with them.</p>"},{"location":"abstract-vector-spaces/#thats-a-vector-space","title":"That's a vector space?","text":"<p>Let's consider the set of all polynomials of degree \\(n \\leq 2\\). We'll denote this set as \\(\\mathcal{P}^2\\).</p> <p>Let's grab three elements of \\(\\mathcal{P}^2\\); call them \\(p_1(x)\\), \\(p_2(x)\\), and \\(p_3(x)\\).</p> <p>Is \\(\\mathcal{P}^2\\) a vector space? Let's check:</p> <ol> <li>Does the order matter when adding two elements, like \\(p_1(x) + p_2(x)\\)? No, so check!</li> <li>If we add more than two elements together, does the order in which we do things matter? Nope, so check!</li> <li>Is there a weird element that leaves other elements unchanged when adding it to them? With all coefficients equal to 0, we just get zero, so zero is technically a polynomial in the set, so check!</li> <li>Does every polynomial have an opposite, so that if we add it with its opposite we get zero? If we take a polynomial, copy it, and reverse the signs of all coefficients of the copy, then we get exactly such an opposite. So check!</li> <li>If we multiply any polynomial with the scalar 1, we get that same polynomial back. So check!</li> <li>Does the distributive rule for sums of scalars work as expected? \\((a + b)p_1(x) = ap_1(x) + bp_1(x)\\), so yup, and check!</li> <li>Does the order matter when multiplying with more than one scalar? \\((ab)p_1(x) = abp_1(x) = a(bp_1(x))\\) \u2014 it does not, so check!</li> <li>Does the distributive rule for sums of elements work as expected? \\(a(p_1(x) + p_2(x)) = ap_1(x) + ap_2(x)\\), so yes it does \u2014 check!</li> </ol> <p>Why, look at that! It seems that our set is a vector space.</p> <p>\"Okay,\" you say. \"So what?\"</p> <p>Well, now that we know that these polynomials really are just vector spaces in fancy outfits, we can apply all we know about \"normal\" vector spaces to them.</p>"},{"location":"abstract-vector-spaces/#references","title":"References","text":"<p>[1] T. Smits. (2023). Abstract Linear Algebra [Online]. Available: https://www.math.ucla.edu/~tsmits/115coursenotes.pdf</p>"},{"location":"hilbert-spaces/","title":"Hilbert spaces","text":"<p>The mathematics of quantum mechanics is done in a special type of vector space called a Hilbert space.</p> <p>The following is an incomplete description of what Hilbert spaces actually are, as the complete formalism is (so far) out of scope for what we're trying to accomplish. Whenever we say \"Hilbert space\" throughout these pages, we actually mean the \\(L^2\\) space (the set of all square-integrable functions; see 3.1 of [1]). We will denote this space with \\(\\mathcal{H}\\), and it's the properties of this space that we're talking about below.</p>"},{"location":"hilbert-spaces/#vectors-in-hilbert-space","title":"Vectors in Hilbert space","text":"<p>As mentioned in the section about abstract vector spaces, the vectors of \\(\\mathcal{H}\\) aren't the familiar list-of-numbers we learn about in our basic linear algebra courses. Instead, they are complex functions, usually denoted by the uppercase Greek letter \\(\\Psi\\) (pronounced \"psi\") that map real scalars to complex scalars.</p> <p>By the way, from now on, whenever we mention \"scalars\", we mean complex scalars. Also, remember that the real numbers are a subset of the complex numbers. In other words, just because something is a complex number, it doesn't mean that it for sure contains \\(i\\).</p> <p>Quantum mechanics is \u2014 suprise, surprise \u2014 a branch of physics. We therefore generally want for the symbols we use to have real, physical interpretations. The objects or entities we will talk about tend to be particles, or rather, the state of particles.</p> <p>Now, repeat after me:</p> <p>Quantum states</p> <p>The state of a quantum particle, i.e. a quantum state, is a ray in complex Hilbert space.</p> <p>In order to explain what that means, we'll start off by being sloppy. Incorrectly, we could say that a vector in \\(\\mathcal{H}\\) is a quantum state, and usually we would denote such a thing as \\(\\ket{\\psi}\\).(1)</p> <ol> <li>Don't worry about the wonky notation just yet; it's called Bra-Ket notation and we'll talk about that in the next section.</li> </ol> <p>However, for reasons we have yet to get in to, if we take a scalar \\(\\lambda\\) (remember, scalars are complex from now on), and multiply that with the state \\(\\ket{\\psi}\\):</p> \\[ \\lambda\\ket{\\psi}, \\] <p>that expression has absolutely no physically observable difference to the original state \\(\\ket{\\psi}\\). Therefore, a single quantum state can be represented by an infinitude of different vectors, as long as the difference between those vectors is just a scalar factor \\(\\lambda\\).</p> <p>The reason this matters is because if you were to answer the question \"what is a quantum state\" with \"a vector in Hilbert space\", that would be comparable to answering the question \"what is a line\" with \"that dot over there\". Well, not really, because a vector has a direction too, but the point remains.</p>"},{"location":"hilbert-spaces/#inner-products","title":"Inner products","text":"<p>We are used to being able to take the dot product of any two vectors in the context of \"normal\" vector spaces. In \\(\\mathcal{H}\\), the corresponding operation is the inner product.</p> <p>\"A Hilbert space is a vector space equipped with an inner product such that the norm, which is defined as \\(|f| = \\sqrt{f \\cdot f}\\), turns the space into a complete metric space\"</p>"},{"location":"hilbert-spaces/#references","title":"References","text":"<p>[1] D. J. Griffiths, D. F. Schroeter, Introduction to Quantum Mechanics, 3rd ed. Cambridge, U.K.: Cambridge Univ. Press, 2018, doi: 10.1017/9781316995433.</p>"}]}