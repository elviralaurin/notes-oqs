{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>These notes are my attempt to understand quantum mechanics and open quantum systems. They are written in an effort to be easily understood rather than condensed into a clean set of definitions and proofs(1). As a result, they tend to get a bit wordy, so be warned.</p> <ol> <li>There is a cheatsheet though!</li> </ol> <p>Please note that I am writing these notes as I myself am learning the topic. I am no expert in any sense of the word.</p> <p>All mistakes are my own. If you spot an error or disagree with anything, please leave a comment or a pull request in the GitHub repository for this project.</p> <p>These notes are unfinished! I am continuously updating them as I learn.</p>"},{"location":"abstract-vector-spaces/","title":"Abstract vector spaces","text":"<p>Why is this relevant?</p> <p>The language of quantum mechanics is linear algebra, and everything we do is done in special vector spaces called Hilbert spaces. Hilbert spaces can be infinite-dimensional (don't worry, we'll get there), and the \"vectors\" we work with aren't really vectors in the sense you'd learn about in a basic linear algebra course. Instead, they are functions.</p>"},{"location":"abstract-vector-spaces/#what-we-mean-by-abstract","title":"What we mean by \"abstract\"","text":"<p>Abstract linear algebra is what happens when we do normal linear algebra, but we stop caring about the details of what a vector \"actually\" is. Instead, we just formulate a set of definitions (the same ones as in normal linear algebra, in fact), and base all our logic on those definitions - we make no additional assumptions about the underlying mathematical objects. In other words, we take the findings of normal linear algebra and abstract them so that we are able to apply them to a wider range of objects and topics.</p> <p>For example, consider the vector space \\(R^3\\). It contains any vector in any direction you can literally think of. You can add any two (or more) vectors, stretch or shrink them by multiplying them with scalars, all to your heart's content. Also, you can condense the information of a vector space into just a set of basis vectors, which you can then use as a kind of common language to express any vector in the space. The basis vectors can be arbitrarily chosen, as long as there is at least one representative of each direction of the space, which in math is called linear independence.</p> <p>If we play around with vectors like this for a while, we will start noticing certain properties that seem to be universal for all vector spaces. Formally, there are eight such properties, and we'll use them as axioms for defining what a vector space actually is.</p>"},{"location":"abstract-vector-spaces/#what-makes-a-vector-space","title":"What makes a vector space?","text":"<p>Vector spaces</p> <p>A vector space is a set \\(V\\), that has an addition operation (\\(+\\)) and a scalar multiplication operation (\\(\\cdot\\)), for which the following eight axioms hold (assume that all symbols with an arrow \u2014 e.g. \\(\\vec{v}\\) \u2014 are elements of the set \\(V\\), and all (lowercase) symbols without an arrow \u2014 e.g. \\(a\\) \u2014 are scalars)[1]:</p> <ol> <li>When adding elements, the order of the arguments doesn't matter: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\)</li> <li>When adding more than two arguments, it doesn't matter which two arguments we add together first: \\((\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\\)</li> <li>There is a special element \\(\\vec{0} \\in V\\) that, when added to any other element \\(\\vec{v}\\), leaves \\(\\vec{v}\\) unchanged: \\(\\vec{v} + \\vec{0} = \\vec{v}\\)</li> <li>Every single element \\(\\vec{v}\\) has an opposite \\(-\\vec{v} \\in V\\), so that when you add them together, you get \\(\\vec{0}\\): \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\)</li> <li>When multiplying the scalar \\(1\\) with any element \\(\\vec{v}\\), that element remains unchanged: \\(1 \\cdot \\vec{v} = \\vec{v}\\)</li> <li>There is a distributive rule for sums of scalars like so: \\((a + b)\\cdot\\vec{v} = a \\cdot \\vec{v} + b \\cdot \\vec{v}\\)</li> <li>When multiplying multiple scalars with elements of \\(V\\), the order doesn't matter: \\((a \\cdot b) \\cdot \\vec{v} = a \\cdot (b \\cdot \\vec{v})\\)</li> <li>There is a distributive rule for sums of elements like so: \\(a \\cdot (\\vec{v} + \\vec{u}) = a \\cdot \\vec{v} + a \\cdot \\vec{u}\\)</li> </ol> <p>I'm playing a little fast and loose with the term \"scalar\" here. At this point, just assume that a scalar is a real number.</p> <p>All of these properties probably feel quite trivial. There are some interesting things to note though. First, the way we've written it, whenever we use scalar multiplication we always write the scalar on the left and the element of \\(V\\) on the right. Second \u2014 and most important \u2014 we're really not making any assumptions about the elements of \\(V\\) at all. What this list of properties is really telling us are expectations about how the operations of addition and scalar multiplication behave. So what makes a vector space a vector space really isn't the vectors themselves; it's what we can do with them.</p>"},{"location":"abstract-vector-spaces/#thats-a-vector-space","title":"That's a vector space?","text":"<p>Let's consider the set of all polynomials of degree \\(n \\leq 2\\). We'll denote this set as \\(\\mathcal{P}^2\\).</p> <p>Let's grab three elements of \\(\\mathcal{P}^2\\); call them \\(p_1(x)\\), \\(p_2(x)\\), and \\(p_3(x)\\).</p> <p>Is \\(\\mathcal{P}^2\\) a vector space? Let's check:</p> <ol> <li>Does the order matter when adding two elements, like \\(p_1(x) + p_2(x)\\)? No, so check!</li> <li>If we add more than two elements together, does the order in which we do things matter? Nope, so check!</li> <li>Is there a weird element that leaves other elements unchanged when adding it to them? With all coefficients equal to 0, we just get zero, so zero is technically a polynomial in the set, so check!</li> <li>Does every polynomial have an opposite, so that if we add it with its opposite we get zero? If we take a polynomial, copy it, and reverse the signs of all coefficients of the copy, then we get exactly such an opposite. So check!</li> <li>If we multiply any polynomial with the scalar 1, we get that same polynomial back. So check!</li> <li>Does the distributive rule for sums of scalars work as expected? \\((a + b)p_1(x) = ap_1(x) + bp_1(x)\\), so yup, and check!</li> <li>Does the order matter when multiplying with more than one scalar? \\((ab)p_1(x) = abp_1(x) = a(bp_1(x))\\) \u2014 it does not, so check!</li> <li>Does the distributive rule for sums of elements work as expected? \\(a(p_1(x) + p_2(x)) = ap_1(x) + ap_2(x)\\), so yes it does \u2014 check!</li> </ol> <p>Why, look at that! It seems that our set is a vector space.</p> <p>\"Okay,\" you say. \"So what?\"</p> <p>Well, now that we know that the set of these polynomials really is just a vector space in a fancy outfit, we can apply all we know about \"normal\" vector spaces to it. We would probably not normally think of polynomials as vectors, but we have just shown that we just might as well look at them through that lense. And maybe we'll spot something interesting when we do.</p>"},{"location":"abstract-vector-spaces/#references","title":"References","text":"<p>[1] T. Smits. (2023). Abstract Linear Algebra [Online]. Available: https://www.math.ucla.edu/~tsmits/115coursenotes.pdf</p>"},{"location":"cheatsheet/","title":"Cheatsheet for Quantum Mechanics","text":"<p>This page originally acted as an intermediary for my own knowledge \u2014 writing down what I learn as I learn it \u2014 so that I could try to explain stuff in my own words later in the form of the rest of the notes on this website.</p>"},{"location":"cheatsheet/#bras-kets-and-the-inner-product","title":"Bras, kets, and the inner product","text":"<p>Let \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\) be two state vectors in a Hilbert space \\(\\mathcal{H}\\).</p> <p>Properties of the inner product</p> \\[ \\begin{align}     &amp;\\braket{\\psi | \\phi} = c, c \\in \\mathbb{C},\\\\\\\\     &amp;\\braket{\\psi | \\phi} = \\bigl(\\braket{\\phi | \\psi}\\bigr)^* = \\braket{\\phi | \\psi}^*,\\\\\\\\     &amp;\\bra{\\psi} \\Bigl(a\\ket{\\phi_1} + b\\ket{\\phi_2}\\Bigr) = a\\braket{\\psi | \\phi_1} + b\\braket{\\psi | \\phi_2},\\\\\\\\     &amp;\\Bigl(a\\bra{\\psi_1} + b\\bra{\\psi_2}\\Bigr)\\ket{\\phi} = a^*\\braket{\\psi_1 | \\phi} + b^*\\braket{\\psi_2 | \\phi}. \\end{align} \\] <p>Inner product of a ket with itself</p> \\[ \\begin{align}     &amp;\\braket{\\psi | \\psi} = r, \\text{ where }r \\in \\mathbb{R} \\text{ and } r \\geq 0,\\text{ and}\\\\     &amp;\\braket{\\psi | \\psi} = 0, \\text{ iff } \\ket{\\psi} = \\ket{\\text{null}}. \\end{align} \\]"},{"location":"cheatsheet/#operators","title":"Operators","text":"<p>A measurable, physical property \\(\\mathcal{A}\\) is called an observable and is described by an operator \\(\\hat{A}\\) acting on the state space \\(\\mathcal{H}\\).</p> <p>The result of \\(\\hat{A}\\) acting on a ket \\(\\ket{\\psi}\\) is another ket \\(\\ket{\\psi'}\\), which is also in the state space:</p> \\[ \\hat{A}\\ket{\\psi} = \\ket{\\psi'},\\quad\\quad\\ket{\\psi'} \\in \\mathcal{H}. \\] <p>Let \\(\\hat{A}\\), \\(\\hat{B}\\), and \\(\\hat{C}\\) be linear operators acting on \\(\\mathcal{H}\\).</p> <p>Operator addition</p> <p>Operator addition is:</p> <ul> <li>associative: \\((\\hat{A} + \\hat{B}) + \\hat{C} = \\hat{A} + (\\hat{B} + \\hat{C})\\);</li> <li>commutative: \\(\\hat{A} + \\hat{B} = \\hat{B} + \\hat{A}\\).</li> </ul> <p>Operator multiplication</p> <p>Operator multiplication is defined by how operators act on kets:</p> \\[ (\\hat{A}\\hat{B})\\ket{\\psi} = \\hat{A}(\\hat{B}\\ket{\\psi}) = \\hat{A}\\ket{\\psi'} \\] <p>Operator multiplication is therefore:</p> <ul> <li>associative: \\(\\hat{A}(\\hat{B}\\hat{C}) = (\\hat{A}\\hat{B})\\hat{C}\\);</li> <li>NOT necessarily commutative: \\(\\hat{A}\\hat{B} \\neq \\hat{B}\\hat{A}\\).</li> </ul> <p>Conjugation of the matrix element</p> <p>For a matrix element \\(\\bra{\\psi} \\hat{A} \\ket{\\phi} \\in \\mathbb{C}\\), it is the case that:</p> \\[ \\bra{\\psi} \\hat{A} \\ket{\\phi} = \\bra{\\phi} \\hat{A}^\\dagger \\ket{\\psi}^* \\] <p>Inverse operators</p> <p>The inverse \\(\\hat{A}^{-1}\\) of an operator \\(\\hat{A}\\) is defined by:</p> \\[ \\hat{A}^{-1}\\hat{A} = \\hat{A}\\hat{A}^{-1} = \\mathbb{I}, \\] <p>where \\(\\mathbb{I}\\) is the identity matrix.</p> <p>Hermitian operators</p> <p>An operator \\(\\hat{A}\\) is Hermitian if it is equal to its adjoint:</p> \\[ \\hat{A} = \\hat{A}^\\dagger. \\] <p>Unitary operators</p> <p>An operator \\(\\hat{U}\\) is unitary if it is equal to its inverse:</p> \\[ \\hat{U} = \\hat{U}^{-1}, \\] <p>or, equivalently,</p> \\[ \\hat{U}^\\dagger\\hat{U} = \\hat{U}\\hat{U}^\\dagger = \\mathbb{I}. \\] <p>Adjoint operator</p> <p>For two operators \\(\\hat{A}\\) and \\(\\hat{B}\\), the following are true:</p> \\[ \\begin{align}     (\\hat{A}^\\dagger)^\\dagger = \\hat{A},\\\\     (a\\hat{A})^\\dagger = a^*\\hat{A}^\\dagger,\\\\     (\\hat{A} + \\hat{B})^\\dagger = \\hat{A}^\\dagger + \\hat{B}^\\dagger,\\\\     (\\hat{A}\\hat{B})^\\dagger = \\hat{B}^\\dagger\\hat{A}^\\dagger. \\end{align} \\] <p>Finding the expansion coefficients of a state</p> <p>The expansion coefficients \\(c_i\\) of a state vector \\(\\ket{\\psi}\\) in an orthonormal basis \\(\\{u_i\\}\\) can be found by projecting \\(\\ket{\\psi}\\) onto the basis:</p> \\[ \\braket{u_j | \\psi} = c_i. \\] <p>Adjoint of outer product</p> <p>Taking the adjoint of an outer product reverses the terms:</p> \\[ \\bigl(\\ket{\\phi}\\bra{\\psi}\\bigr)^\\dagger = \\ket{\\psi}\\bra{\\phi}. \\] <p>Resolution of the identity</p> <p>The identity operator \\(\\mathbb{I}\\) can be written as an outer product like so:</p> \\[ \\mathbb{I} = \\sum_i{\\ket{u_i}\\bra{u_i}}. \\] <p>Writing a ket in a particular orthonormal basis</p> <p>A ket \\(\\ket{\\psi} \\in \\mathcal{H}\\) can be written in the orthonormal basis \\(\\{\\ket{u_i}\\}\\) like so:</p> \\[ \\ket{\\psi} = \\sum_i{c_i\\ket{u_i}}, \\quad\\quad c_i = \\braket{u_i | \\psi}. \\] <p>Writing a bra in a particular orthonormal basis</p> <p>A bra \\(\\bra{\\psi} \\in \\mathcal{H}^*\\) can be written in the orthonormal basis \\(\\{\\ket{u_i}\\}\\) like so:</p> \\[ \\bra{\\psi} = \\sum_i{c_i^*\\bra{u_i}}, \\quad\\quad c_i^* = \\braket{\\psi | u_i}. \\] <p>Writing an operator in a particular orthonormal basis</p> <p>An operator \\(\\hat{A}\\) can be written in the orthonormal basis \\(\\{\\ket{u_i}\\}\\) like so:</p> \\[ \\hat{A} = \\sum_{ij}{A_{ij}\\ket{u_i}\\bra{u_j}}, \\quad\\quad A_{ij} = \\bra{u_i}\\hat{A}\\ket{u_j}. \\] <p>Born rule</p> <p>For a system in state \\(\\ket{\\psi}\\), the probability of getting a particular measurement outcome \\(\\lambda_n\\) of observable \\(\\hat{A}\\) is given by the Born rule:</p> \\[ P(\\lambda_n) = |\\braket{\\lambda_n | \\psi}|^2 = |c_n|^2. \\] <p>Expectation value</p> <p>The expectation value \\(\\braket{\\hat{A}}_\\psi\\) of an observable \\(\\hat{A}\\) in a normalized state \\(\\ket{\\psi}\\) is given by:</p> \\[ \\braket{\\hat{A}}_\\psi = \\sum_n{\\lambda_n P(\\lambda_n)} = \\sum_n{\\lambda_n |\\braket{\\lambda_n | \\psi}|^2} = \\sum_n{\\lambda_n |c_n|^2} = \\bra{\\psi}\\hat{A}\\ket{\\psi}. \\]"},{"location":"check-yourself/","title":"Check yourself","text":"<p>Dimensionality of composite systems</p> <p>Assume you have a quantum system \\(\\mathcal{H}\\) consisting of one qubit (a 2D Hilbert space) and one qutrit (a 3D Hilbert space). What is the dimension of \\(\\mathcal{H}\\)?</p> Answer <p>The qubit has two dimensions, and therefore has two basis vectors. The qutrit has three dimensions, and therefore has three dimensions.</p> <p>A composite system like \\(\\mathcal{H}\\) is formed as the tensor product of the component spaces. If the qubit has basis vectors \\(\\{b_i\\}\\), and the qutrit basis vectors \\(\\{t_j\\}\\), \\(\\mathcal{H}\\) has basis vectors of the form \\(\\ket{b_i} \\otimes \\ket{t_j}\\), where \\(i\\) and \\(j\\) are iterated over all possible combinations. So \\(\\mathcal{H}\\) has \\(2 \\times 3 = 6\\) basis vectors, resulting in it having 6 dimensions.</p> <p>Orthonormal basis representations</p> <p>Write the following in the orthonormal basis \\(\\{\\ket{u_i}\\}\\):</p> <ol> <li>the ket \\(\\ket{\\psi}\\)</li> <li>the bra \\(\\bra{\\phi}\\)</li> <li>the operator \\(\\hat{A}\\)</li> </ol> Answer \\[ \\begin{align}     \\ket{\\psi} = \\sum_i{c_i\\ket{u_i}}, \\quad\\quad c_i = \\braket{u_i | \\psi},\\\\     \\bra{\\phi} = \\sum_i{c_i^*\\bra{u_i}}, \\quad\\quad c_i^* = \\braket{\\phi | u_i},\\\\     \\hat{A} = \\sum_{ij}{A_{ij}\\ket{u_i}\\bra{u_j}}, \\quad\\quad A_{ij} = \\bra{u_i}\\hat{A}\\ket{u_j}. \\end{align} \\] <p>The importance of Hermitian operators</p> <p>What does it mean for an operator to be Hermitian? Why are all operators in QM Hermitian?</p> Answer <p>An operator \\(\\hat{A}\\) is Hermitian if it is equal to its adjoint:</p> \\[ \\hat{A} = \\hat{A}^\\dagger. \\] <p>Hermitian operators have two important properties: they have real eigenvalues, and their eigenstates are orthogonal.</p> <p>The result of a measurement of an observable will always be one of the corresponding operator's eigenvalues. In order for this to make sense for physical systems, this implies that the corresponding operator must have real eigenvalues.</p> <p>Furthermore, we need the eigenstates of the operators to be orthogonal because otherwise we allow for measurement outcomes which are linear combinations of other states, i.e. in a superposition. While systems can be in a superpositioned state before measurement, the act of measuring cannot resolve to a superpositioned state. Once we open the box containing Schr\u00f6dinger's cat, the cat is either dead or alive, but not both.</p> <p>To ensure these properties, we represent physical observables with Hermitian operators.</p> <p>Result of the inner product</p> <p>What can be said about:</p> <ol> <li>the result of the inner product of two different state vectors \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\)?</li> <li>the result of the inner product of a state vector \\(\\ket{\\psi}\\) with itself?</li> <li>the state vector which, if you take the inner product of it with itself, results in 0?</li> <li> <p>two different state vectors which, if you take the inner product of them, results in 0?</p> Answer <p>1. The result of the inner product of two different state vectors is a complex number: \\(\\braket{\\psi | \\phi} = c \\in \\mathbb{C}\\).</p> <p>2. The result of the inner product of a state vector with itself is a real, positive number: \\(\\braket{\\psi | \\psi} = a \\in \\mathbb{R}_{\\geq 0}\\).</p> <p>3. If the result of the inner product of a state vector with itself is 0, then that state vector is the null ket \\(\\ket{\\psi}\\): \\(\\braket{\\psi | \\psi} = 0 \\Rightarrow \\ket{\\psi} = \\ket{\\text{null}}\\)</p> <p>4. If the result of the inner product of two different state vectors is 0, then the two state vectors are orthogonal: \\(\\braket{\\psi | \\phi} = 0 \\land \\ket{\\psi} \\neq \\ket{\\phi} \\Rightarrow \\ket{\\psi} \\perp \\ket{\\phi}\\).</p> </li> </ol> <p>Measuring a quantum system that is in an eigenstate</p> <p>Say you want to measure some property with corresponding operator \\(\\hat{A}\\) of a system that is in state \\(\\ket{\\psi} = \\ket{\\lambda_0}\\), where \\(\\ket{\\lambda_0}\\) is one of the eigenstates of \\(\\hat{A}\\). What are the possible outcomes of this measurement? Assume normalized eigenstates.</p> Answer <p>The Born rule gives us:</p> \\[ P(\\lambda_n) = |\\braket{\\lambda_n | \\psi}|^2, \\] <p>where \\(\\ket{\\psi}\\) is the current state of the system. So if \\(\\ket{\\psi} = \\ket{\\lambda_0}\\), we get:</p> \\[ P(\\lambda_0) = |\\braket{\\lambda_0 | \\lambda_0}|^2 = |1|^2 = 1, \\] <p>since the eigenvectors are normalized.</p> <p>Since the probabilities of the total set of possible outcomes must sum to 1, this means that all other eigenstates have a zero probability.</p> <p>Example: probabilities and the expectation value</p> <p>Say we measure the observable \\(\\hat{A}\\) of a system in state \\(\\ket{\\psi}\\). \\(\\hat{A}\\) has two eigenvalues, \\(\\lambda_+\\) and \\(\\ket{\\lambda_-}\\), such that the eigenvalue equations are given by:</p> \\[ \\begin{align}     \\hat{A}\\ket{\\lambda_+} = \\ket{\\lambda_+},\\\\     \\hat{A}\\ket{\\lambda_-} = -\\ket{\\lambda_-}. \\end{align} \\] <p>The current state is given by \\(\\ket{\\psi} = \\frac{1}{\\sqrt{2}}(\\ket{\\lambda_+} + \\ket{\\lambda_-})\\).</p> <p>Calculate the following:</p> <ol> <li>The probability of measuring \\(\\lambda_+\\)</li> <li>The probability of measuring \\(\\lambda_-\\)</li> <li>The expectation value \\(\\braket{\\hat{A}}_\\psi\\)</li> </ol> Answer <p>We use the Born rule to find the probabilities:</p> \\[ \\begin{align}     P(\\lambda_+) = |\\braket{\\lambda_+ | \\psi}|^2 = |\\braket{\\lambda_+ | \\psi}|^2 \\end{align} \\]"},{"location":"fundamentals/","title":"Fundamentals","text":"<p>On the following pages, we start off with the basics of quantum mechanics (QM). We will be a little loose with our definitions at first, but hopefully we'll be able to become more rigorous as we get a better understanding for what we're doing.</p>"},{"location":"fundamentals/change-of-basis/","title":"Change of basis","text":"<p>A state vector \\(\\ket{\\psi}\\) can be written in terms of a set of basis vectors \\(\\{\\ket{u_i}\\}\\), which spans \\(\\mathcal{H}\\).</p> <p>In the vast majority of cases, we'll be dealing with orthonormal bases:</p> <p>Orthonormal basis</p> <p>A set of vectors \\(\\{\\ket{u_i}\\}\\) is an orthonormal basis to \\(\\mathcal{H}\\) if the vectors are mutually orthogonal (\\(\\braket{u_i | u_j} = 0, i \\neq j\\)) and all are normalized (\\(\\braket{u_i | u_i} = 1\\)).</p> <p>We can summarize the properties of an orthonormal basis using the Kroenecker delta \\(\\delta_{ij}\\), which is defined as(1):</p> <ol> <li>I like to think about it as \"equals\" for the indices \\(i\\) and \\(j\\), returning 1 for \"true\" and 0 for \"false\": <code>i == j ? 1 : 0</code></li> </ol> \\[ \\delta_{ij} = \\begin{cases}     1, i = j,\\\\     0, i \\neq j. \\end{cases} \\] <p>Thus, a basis \\(\\{\\ket{u_i}\\}\\) is orthonormal if \\(\\braket{u_i | u_j} = \\delta_{ij}\\).</p> <p>Every ket \\(\\ket{\\psi} \\in \\mathcal{H}\\) has a unique expansion of the basis vectors:</p> \\[ \\ket{\\psi} = \\sum_i{c_i \\ket{u_i}}, \\] <p>where \\(c_i\\) is the expansion coefficient for the \\(i\\)th basis vector \\(\\ket{u_i}\\). This means that if the basis is known, we only need the set of expansion coefficients \\(c_i\\) to uniquely identify a state vector.</p> <p>Determining the expansion coefficients</p> <p>Given a basis \\(\\{\\ket{u_i}\\}\\) spanning \\(\\mathcal{H}\\), we can find the expansion coefficients of a state \\(\\ket{\\psi} \\in \\mathcal{H}\\) as follows:</p> \\[ \\begin{align}     \\braket{u_j | \\psi} =\\\\     \\bra{u_j} \\Bigl(\\sum_i{c_i \\ket{u_i}}\\Bigr) =\\\\     \\sum_i{c_i \\braket{u_j | u_i}} =\\\\     \\sum_i{c_i \\delta_{ij}} =\\\\     c_i. \\end{align} \\] <p>Note that in the second to last step, \\(\\delta_{ij} = 0\\) for every single term for which \\(i \\neq j\\). This leaves only one term in the sum: the one where \\(i = j\\), which makes \\(\\delta_{ij} = 1\\), which results in \\(c_i\\delta_{ij} = c_i \\cdot 1 = c_i\\).</p> <p>Say we have the ket \\(\\ket{\\psi}\\). We can write it in the orthonormal basis \\(\\{\\ket{u_i}\\}\\) like so:</p> \\[ \\ket{\\psi} = \\sum_i{c_i\\ket{u_i}}, \\quad\\quad c_i = \\braket{u_i | \\psi}. \\] <p>We now know how to represent a given ket in a specific basis. How about the bras?</p> <p>Say we have a ket \\(\\ket{\\psi} = \\sum_i{c_i\\ket{u_i}} \\in \\mathcal{H}\\), with expansion coefficients \\(c_i = \\braket{u_i | \\psi}\\). The corresponding a bra \\(\\bra{\\psi} \\in \\mathcal{H}^*\\) can then be written:</p> \\[ \\begin{align}     \\bra{\\psi} &amp;=\\\\     \\bra{\\psi}\\mathbb{I} &amp;=\\\\     \\bra{\\psi} \\sum_i{\\Bigl(\\ket{u_i}\\bra{u_i}\\Bigr)} &amp;=\\\\     \\sum_i{\\Bigl(\\braket{\\psi | u_i}\\bra{u_i}\\Bigr)} &amp;=\\\\     \\sum_i{\\Bigl(\\braket{u_i | \\psi}^*\\bra{u_i}\\Bigr)} &amp;=\\\\     \\sum_i{\\Bigl(c_i^*\\bra{u_i}\\Bigr)}. \\end{align} \\] <p>Finally, we want to be able to express operators in terms of a specific basis.</p> <p>Writing an operator in a particular orthonormal basis</p> <p>We can write an operator \\(\\hat{A}\\ket{\\psi} = \\ket{\\psi'}\\) in a particular orthonormal basis \\(\\{\\ket{u_i}\\}\\) like so(1):</p> <ol> <li>See [1] for proof.</li> </ol> \\[ \\hat{A} = \\sum_{ij}{\\ket{u_i}\\bra{u_i}\\hat{A}\\ket{u_j}\\bra{u_j}}, \\] <p>which, if we let \\(A_{ij}\\) be the matrix element \\(\\bra{u_i}\\hat{A}\\ket{u_j}\\), becomes:</p> \\[ \\hat{A} = \\sum_{ij}{A_{ij}\\ket{u_i}\\bra{u_j}}. \\]"},{"location":"fundamentals/change-of-basis/#references","title":"References","text":"<p>[1] Professor M does Science, Cambridge, U.K. Matrix formulation of quantum mechanics. (June 24, 2020). Accessed: Aug. 20, 2024. [Online Video]. Available: https://youtu.be/wIwnb1ldYTI.</p>"},{"location":"fundamentals/dual-space/","title":"Dual space","text":"<p>\"To every ket, there corresponds a bra\" \u2014 B. Monserrat[1]</p> <p>Just like the kets live in state space \\(\\mathcal{H}\\), the bras(1) live in a space we denote \\(\\mathcal{H}^*\\). This space is commonly called the dual space(2).</p> <ol> <li>If this is term is unfamiliar to you, you should check out the notes on the inner product first.</li> <li>It's actually a bit misleading to call it the dual space, as all vector spaces has a dual space. When we talk about dual spaces in QM, at least in these notes, what we mean is really just the space of all bras, i.e. the linear functionals to the kets. Search for the \"Riesz representation theorem\" for more information.</li> </ol> <p>Note</p> <p>The dual space \\(\\mathcal{H}^*\\) is also a Hilbert space!</p> <p>Recall that in Euclidean space, the dot product of two vectors \\(\\bar{u}\\) and \\(\\bar{v}\\) can be thought of as multiplying the corresponding components and adding up the result:</p> \\[ \\bar{u} \\cdot \\bar{v} = [u_1, u_2, \\dots, u_n] \\cdot [v_1, v_2, \\dots, v_n] = \\sum_{i=1}^n{u_iv_i}. \\] <p>However, we can also think about it as a matrix multiplication where the first factor is a row vector and the second a column vector:</p> \\[ \\bar{u} \\cdot \\bar{v} = \\begin{bmatrix}     u_1 &amp; u_2 &amp; \\dots &amp; u_n \\end{bmatrix} \\begin{bmatrix}     v_1\\\\     v_2\\\\     \\vdots\\\\     v_n \\end{bmatrix} = \\sum_{i=1}^n{u_iv_i}. \\] <p>If vectors are thought of as column vectors by default, we can say that we \"flipped\" the first vector to turn it into a row vector in order to calculate the dot product. Mathematically, this \"flipping\" of column vectors to row vectors is done by taking the transpose of the vector. In other words, a column vector \\(\\bar{v}\\) has a corresponding row vector which is found by taking the transpose of the original vector: \\(\\bar{v}^T\\).</p> <p>The relationship between row and column vectors is similar to that of bras and kets, with one additional difference:</p> <p>Column vs. row vector, ket vs. bra</p> <p>Let \\(a_r \\in \\mathbb{R}\\) and \\(a_c \\in \\mathbb{C}\\) be scalars.</p> <p>In Euclidean space, given a column vector \\(a_r\\bar{v} \\in \\mathbb{R}^n\\), the corresponding row vector is given by \\(a_r\\bar{v}^T \\in \\mathbb{R}^{1 \\times n}\\). When going from column vector to row vector, we leave the scalar unchanged.</p> <p>In state space, given a ket \\(a_c\\ket{\\psi} \\in \\mathcal{H}\\), the corresponding bra is given by \\(a_c^*\\bra{\\psi} \\in \\mathcal{H}^*\\). When going from ket to bra, we have to take the complex conjugate of the scalar: \\(a_c\\) becomes \\(a_c^*\\).</p> <p>The reason why we have to take the complex conjugate of any scalars when finding the corresponding bra to a ket comes from the definition of the inner product(1).</p> <ol> <li>For more information on why this is, check out Professor M does Science's video on YouTube from timestamp 10:05 onwards.</li> </ol>"},{"location":"fundamentals/dual-space/#references","title":"References","text":"<p>[1] Professor M does Science, Cambridge, U.K. Dirac Notation: State Space And Dual Space. (May 23, 2020). Accessed: Jul. 3, 2024. [Online Video]. Available: https://www.youtube.com/watch?v=hJoWM9jf0gU.</p>"},{"location":"fundamentals/inner-product/","title":"Inner product","text":"<p>Just like we can take the dot product of two vectors in Euclidean space, we can take what's called the inner product of two kets \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\) in Hilbert space. The inner product of \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\) is written \\(\\braket{\\psi | \\phi}\\), and results in a scalar:</p> \\[ \\text{inner_prod}(\\ket{\\psi}, \\ket{\\phi}) = \\braket{\\psi | \\phi} = c,\\quad\\quad c \\in \\mathbb{C}. \\] <p>Typographically, taking the inner product looks like flipping the first ket and sticking it together with the second ket:</p> \\[ \\ket{\\psi} \\ket{\\phi} \\rightarrow \\bra{\\psi} \\ket{\\phi} \\rightarrow \\braket{\\psi | \\phi} \\] <p>This is no accident. That \"flipped ket\" \\(\\bra{\\phi}\\) is called a bra, and sticking it together with a ket produces a bra-ket \u2014 a bracket. This is all part of Dirac notation, and this idea of brackets is why Dirac notation is also commonly called bra-ket notation.</p> <p>Properties of the inner product</p> <p>The inner product \\(\\braket{\\psi | \\phi}\\) of kets \\(\\ket{\\psi}, \\ket{\\phi} \\in \\mathcal{H}\\) has the following properties:</p> <p>Conjugation</p> <p>We can invert the order of the arguments by taking the complex conjugate of the whole thing: \\(\\braket{\\psi | \\phi} = (\\braket{\\phi | \\psi})^*\\), which is usually just written \\(\\braket{\\phi | \\psi}^*\\).</p> <p>Linearity in the second argument</p> <p>If we multiply the inner product with a scalar \\(a\\), we can \"move the scalar in\" to the second argument like so:</p> \\[ a\\braket{\\psi | \\phi} = \\bra{\\psi} a(\\ket{\\phi}). \\] <p>Furthermore, if the second argument of the inner product is a sum of two kets, say \\(\\ket{\\phi} = \\ket{\\phi_1} + \\ket{\\phi_2}\\), we can \"distribute\" the inner product like so:</p> \\[ \\bra{\\psi} \\Bigl(\\ket{\\phi_1} + \\ket{\\phi_2}\\Bigr) = \\braket{\\psi | \\phi_1} + \\braket{\\psi | \\phi_2}. \\] <p>As you can see from my choice of words above, I like thinking about this property as corresponding to distributing over parentheses in \"regular algebra\":</p> <p></p> <p>Antilinearity in the first argument</p> <p>If the first argument is a linear superposition, we can distribute the second argument over the first provided that we take the complex conjugate of any coefficients:</p> \\[ \\Bigl(a\\bra{\\psi_1} + b\\bra{\\psi_2}\\Bigr) \\ket{\\phi} = a^*\\braket{\\psi_1 | \\phi} + b^*\\braket{\\psi_2 | \\phi} \\] <p>This property is a consequence of the previous two.</p> <p>Positivity</p> <p>The inner product of a ket with itself is a real, positive number that is only equal to zero iff the ket in question is the null ket \\(\\ket{\\text{null}}\\):</p> \\[ \\begin{align}     &amp;\\braket{\\psi | \\psi} \\geq 0,\\newline     &amp;\\braket{\\psi | \\psi} = 0, \\text{ iff } \\ket{\\psi} = \\ket{\\text{null}}. \\end{align} \\] <p>You can find a summary of the above in the cheatsheet.</p>"},{"location":"fundamentals/measurement/","title":"Measurement","text":"<p>The measurement of a quantum system and what happens to the system after a measurement has been done is at the core of the strangeness most people associate with quantum mechanics.</p> <p>What we will find is that it is impossible to predict the outcome of a measurement of a quantum system.</p> <p>This is quite a bold statement. All physical laws we are aware of in the world of classical mechanics are deterministic \u2014 every event has a cause and an effect. If you know everything about the state of something at any point in time (i.e. complete, perfect knowledge, if such a thing could exist), it is possible to retrace all past states, and to predict all future states.</p> <p>When we say that it is impossible to predict the outcome of a quantum measurement, it implies that some things don't have a clear cause and effect. Note that we aren't saying that we just simply don't know how to predict the outcome, we are making the stronger claim of it physically not being possible to know.</p> <p>I don't know about you, but to me, this claim sounds absolutely insane. Hopefully, we'll be able to shed some light on why this is the best conclusion.</p> <p> </p> <p>\"Richard Feynman, one of the big figures in physics, used to say 'No one understands quantum mechanics.' So in some sense, the pressure is off for you guys, because I don't get it and you don't get it and Feynman doesn't get it. The point is, here is my goal: right now, I am the only one who doesn't understand quantum mechanics. In about seven days, all of you will be unable to understand quantum mechanics.\" \u2014 R. Shankar[1]</p>"},{"location":"fundamentals/measurement/#eigenstates-and-eigenvalues","title":"Eigenstates and eigenvalues","text":"<p>Say you have a quantum system of some kind, and you want to measure one of its physical properties. That physical property corresponds to a particular Hermitian operator \\(\\hat{A}\\), and the result of you measuring this property of the system will be one of the eigenvalues of \\(\\hat{A}\\).</p> <p>The eigenvalue equation</p> <p>Let \\(\\hat{A}\\) be an operator. The eigenstates \\(\\ket{\\lambda}\\) and eigenvalues \\(\\lambda\\) of \\(\\hat{A}\\) are such that they obey the eigenvalue equation:</p> \\[ \\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda}. \\] <p>Note that \\(\\ket{\\lambda}\\) is a state vector, while \\(\\lambda\\) is a complex number.</p> <p>The set of eigenvalues \\(\\{\\lambda_i\\}\\) is called the spectrum of \\(\\hat{A}\\).</p>"},{"location":"fundamentals/measurement/#eigenstates-are-only-defined-up-to-a-constant","title":"Eigenstates are only defined up to a constant","text":"<p>Let's say we have an operator \\(\\hat{A}\\) with eigenvalue equation \\(\\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda}\\). Now, let's see what happens if we scale the eigenstate with a factor \\(\\alpha \\in \\mathbb{C}\\):</p> \\[ \\begin{align}     &amp;\\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda},\\\\     &amp;\\ket{\\lambda'} = \\alpha\\ket{\\lambda}, \\alpha \\in \\mathbb{C}.\\\\     &amp;\\Rightarrow\\\\     &amp;\\hat{A}\\ket{\\lambda'} = \\hat{A}(\\alpha\\ket{\\lambda}) = \\alpha\\hat{A}\\ket{\\lambda} = \\alpha(\\lambda\\ket{\\lambda}) = \\alpha\\lambda\\ket{\\lambda} = \\lambda(\\alpha\\ket{\\lambda}) = \\lambda\\ket{\\lambda'}\\\\     &amp;\\Leftrightarrow\\\\     &amp;\\hat{A}\\ket{\\lambda'} = \\lambda\\ket{\\lambda'}. \\end{align} \\] <p>This is just the same eigenvalue equation for the operator \\(\\hat{A}\\) again, with the same eigenvalue \\(\\lambda\\). In other words, if you find an eigenstate \\(\\ket{\\lambda}\\), any multiple \\(\\alpha\\ket{\\lambda}\\) of the eigenstate will also be an eigenstate. This demonstrates that eigenstates are only defined up to a constant \u2014 they only really tell us about a direction in \\(\\mathcal{H}\\), and any state vector in that direction will also be an eigenstate.</p> <p>When we talk about eigenstates without acknowledging this fact, what we tend to mean is the normalized eigenstate. The corresponding eigenvalue will then be the factor \\(\\alpha\\) that makes \\(\\braket{\\lambda | \\lambda} = 1\\).</p>"},{"location":"fundamentals/measurement/#global-phase-factor","title":"Global phase factor","text":"<p>\"Quantum mechanics is independent of a global phase\" - B. Monserrat[2]</p> <p>Since we are dealing with complex vector spaces, there is another fact that we have to keep in mind.</p> <p>Let's say we have an eigenstate \\(\\ket{\\lambda}\\) that is normalized: \\(\\braket{\\lambda | \\lambda} = 1\\), and we scale it by a factor \\(\\alpha = e^{i\\theta}\\)(1).</p> <ol> <li>Recall that a complex number \\(c \\in \\mathbb{C}\\) can be written in polar coordinates as \\(c = re^{i\\theta}\\), where \\(r\\) is the magnitude and \\(\\theta\\) is the phase in radians.</li> </ol> <p>Note that when scaling it by this factor, we're not actually changing the magnitude of \\(\\ket{\\lambda}\\), we're only shifting it by some phase \\(\\theta\\). We get:</p> \\[ \\begin{align}     \\ket{\\lambda'} = e^{i\\theta}\\ket{\\lambda} \\Rightarrow\\\\     \\braket{\\lambda' | \\lambda '} = \\Bigl(e^{-i\\theta}\\bra{\\lambda}\\Bigr)\\Bigl(e^{i\\theta}\\ket{\\lambda}\\Bigr) = e^{-i\\theta}e^{i\\theta}\\braket{\\lambda | \\lambda} = \\braket{\\lambda | \\lambda} = 1. \\end{align} \\] <p>The magnitude of the phase-shifted eigenstate is the same as the original eigenstate. The phase-shifting had no impact on the defining property of the eigenstate.</p> <p>Differentiating eigenstates</p> <p>Two eigenstates \\(\\ket{\\lambda_1}\\) and \\(\\ket{\\lambda_2}\\) are equivalent to each other if:</p> <ul> <li>one is a multiple of the other: \\(\\ket{\\lambda_1} = \\alpha\\ket{\\lambda_2}\\),</li> <li>one is phase-shifted from the other: \\(\\ket{\\lambda_1} = e^{i\\theta}\\ket{\\lambda_2}\\).</li> </ul> <p>In other words, you can multiply an eigenstate by any complex number \\(c = re^{i\\theta} \\in \\mathbb{C}\\), and it will still correspond to the same eigenstate.</p>"},{"location":"fundamentals/measurement/#what-happens-when-we-measure-a-quantum-system","title":"What happens when we measure a quantum system?","text":"<p>As stated before, one of the postulates of quantum mechanics is that the result of a measurement of a physical quantity is one of the eigenvalues of the associated observable. This is what the word \"quantum\" alludes to \u2014 the result of a measurement can only be one of a discrete set of outputs.</p> <p>So we know that if we measure a physical quantity \\(\\mathcal{A}\\), it will result in one of the eigenvalues \\(\\lambda_n\\) of the corresponding operator \\(\\hat{A}\\). But how do we know which \\(\\lambda_n\\) we'll get?</p> <p>The Born rule</p> <p>The measurement of \\(\\mathcal{A}\\) in a system in normalized state \\(\\ket{\\psi}\\) gives eigenvalue \\(\\lambda_n\\) with probability:</p> \\[ P(\\lambda_n) = |\\braket{\\lambda_n | \\psi}|^2, \\] <p>where \\(\\ket{\\lambda_n}\\) is the eigenstate corresponding to eigenvalue \\(\\lambda_n\\)[3].</p> <p>This property is called the Born rule and is another one of the postulates of quantum mechanics. It's gotten its name from the German physicist Max Born, who formulated it in 1926[4].</p> Reflections on the Born rule <p>The Born rule is the thing which introduces the probabilistic nature of quantum mechanics. And it has been demonstrated experimentally time and time again, but the obvious question to me is, how do we know that this isn't just an approximation of some deterministic rule or set of rules?</p> <p>After all, I can very well write a probabilistic model of the outcome of a dice roll and say that \"each possible outcome of the roll of a 6-sided die has a 1 in 6 probability\". And that model would be correct, and be possible to verify through experimentation. But it wouldn't tell the whole story.</p> <p>If I knew everything there was to know about the die: its weight, weight distribution, the exact way in which it was rolled, including its rotation speed and so on, I could use the classical laws of physics to create a deterministic model which could accurately predict the exact result of the roll, provided I had access to all the information. So how can we be so sure that the situation isn't the same in quantum mechanics?</p> <p>So far, perhaps forever, the Born rule is the best we've got when it comes to the prediction of quantum measurements.</p> <p>Now, since we can write a state \\(\\ket{\\psi}\\) in terms of some basis to \\(\\mathcal{H}\\), and we know that the eigenstates of an operator \\(\\hat{A}\\) form such a basis, we can write \\(\\ket{\\psi}\\) like so:</p> \\[ \\ket{\\psi} = \\sum_n{c_n\\ket{\\lambda_n}}, \\quad\\quad c_n = \\braket{\\lambda_n | \\psi}. \\] <p>The Born rule then gives us:</p> \\[ \\begin{align}     P(\\lambda_n) = |\\braket{\\lambda_n | \\psi}|^2 \\Leftrightarrow\\\\     P(\\lambda_n) = |c_n|^2. \\end{align} \\] <p>So another way to express the Born rule is by using the expansion coefficients.</p> <p>The Born rule gives us a means to find the probability distribution of the measurement outcome given a system in state \\(\\ket{\\psi}\\). For each possible outcome, i.e. eigenvalue of the observable \\(\\hat{A}\\) in question, there is an associated probability given by the Born rule.</p> <p>Now, what happens after a measurement has been done?</p> <p>State collapse</p> <p>Let \\(\\lambda_n\\) be the eigenvalue that the measurement of property \\(\\mathcal{A}\\) of the system resulted in. Then, the system will be in state \\(\\ket{\\lambda_n}\\) immediately after the measurement, where \\(\\ket{\\lambda_n}\\) is the eigenstate associated with \\(\\lambda_n\\).</p> <p>So if an isolated quantum system is in a state \\(\\ket{\\psi}\\), the measurement of a physical property \\(\\mathcal{A}\\) of the system will result in one of the eigenvalues \\(\\lambda_n\\) of \\(\\hat{A}\\), where \\(\\hat{A}\\) is the corresponding Hermitian operator to \\(\\mathcal{A}\\). Once the system has been measured, its state will instantaneously become the eigenstate \\(\\lambda_n\\) to the corresponding eigenvalue \\(\\lambda_n\\).</p>"},{"location":"fundamentals/measurement/#expectation-value","title":"Expectation value","text":"<p>Just like in any other application of probability theory, there exists the notion of the expectation value in quantum mechanics as well.</p> <p>Expectation value</p> <p>The expectation value \\(\\braket{\\hat{A}}_{\\psi}\\) of an observable \\(\\hat{A}\\) of a system in a normalized state \\(\\ket{\\psi}\\) is the average value of the measurement outcomes:</p> \\[ \\braket{\\hat{A}}_\\psi = \\sum_n{\\lambda_n P(\\lambda_n)} = \\sum_n{\\lambda_n |\\braket{\\lambda_n | \\psi}|^2} = \\sum_n{\\lambda_n |c_n|^2}. \\] <p>There is also an alternative form of the expectation value that is very commonly used in quantum mechanics:</p> \\[ \\braket{\\hat{A}}_\\psi = \\bra{\\psi}\\hat{A}\\ket{\\psi}. \\] Proof: Alternative form of the expectation value \\[ \\begin{align}     \\bra{\\psi}\\hat{A}\\ket{\\psi} = &amp;&amp; \\\\     \\bra{\\psi}\\mathbb{I}\\hat{A}\\mathbb{I}\\ket{\\psi} = &amp;&amp; \\tiny{\\text{insert resolution of id.}}\\\\     \\bra{\\psi}\\Bigl( \\sum_n{\\ket{\\lambda_n}\\bra{\\lambda_n}} \\Bigr)\\hat{A}\\Bigl( \\sum_m{\\ket{\\lambda_m}\\bra{\\lambda_m}} \\Bigr)\\ket{\\psi} = &amp;&amp; \\tiny{\\text{move sums to beginning}}\\\\     \\sum_{n, m}{\\braket{\\psi | \\lambda_n} \\bra{\\lambda_n}\\hat{A}\\ket{\\lambda_m} \\braket{\\lambda_m | \\psi}} = &amp;&amp; \\tiny{\\text{eigenvalue eq. $\\hat{A}\\ket{\\lambda_m} = \\lambda_m\\ket{\\lambda_m}$}}\\\\     \\sum_{n, m}{\\lambda_m\\braket{\\psi | \\lambda_n} \\braket{\\lambda_n | \\lambda_m} \\braket{\\lambda_m | \\psi}} = &amp;&amp; \\tiny{\\text{basis is orthonormal, $\\braket{\\lambda_n | \\lambda_m} = \\delta_{nm}$}}\\\\     \\sum_{n, m}{\\delta_{nm} \\lambda_m\\braket{\\psi | \\lambda_n} \\braket{\\lambda_m | \\psi}} = &amp;&amp; \\tiny{\\text{only nonzero term is when $n = m \\Rightarrow \\delta_{nm} = 1$}}\\\\     \\sum_n{\\lambda_n \\braket{\\psi | \\lambda_n} \\braket{\\lambda_n | \\psi}} = &amp;&amp; \\\\     \\sum_n{\\lambda_n \\braket{\\lambda_n | \\psi}^* \\braket{\\lambda_n | \\psi}} = &amp;&amp; \\tiny{\\text{for $z \\in \\mathbb{C}$, $zz^* = |z|^2$}}\\\\     \\sum_n{\\lambda_n |\\braket{\\lambda_n | \\psi}|^2}. &amp;&amp; \\end{align} \\] <p>Q.E.D.</p> <p>If we are not working with normalized states, the expectation value is simply given by:</p> \\[ \\braket{\\hat{A}}_\\psi = \\frac{\\bra{\\psi} \\hat{A} \\ket{\\psi}}{\\braket{\\psi | \\psi}}. \\] <p>The expectation value does not necessarily equal any actual measurement outcome. </p>"},{"location":"fundamentals/measurement/#references","title":"References","text":"<p>[1] YaleCourses, Yale Univ. Press, New Haven, CT, USA. 19. Quantum Mechanics I: The key experiments and wave-particle duality. (March 24, 2011). Accessed: Aug. 22, 2024. [Online Video]. Available: https://youtu.be/uK2eFv7ne_Q?feature=shared.</p> <p>[2] Professor M does Science, Cambridge, U.K. Eigenvalues and eigenstates in quantum mechanics. (July 1, 2020). Accessed: Aug. 21, 2024. [Online Video]. Available: https://youtu.be/p1zg-c1nvwQ?feature=shared.</p> <p>[3] Professor M does Science, Cambridge, U.K. Measurements in quantum mechanics || Concepts. (Nov. 11, 2020). Accessed: Aug. 23, 2024. [Online Video]. Available: https://youtu.be/u1R3kRWh1ek?feature=shared.</p> <p>[4] N. P. Landsman, \"Born Rule and its Interpretation,\" in Compendium of Quantum Physics, D. Greenberger, K. Hentschel, F. Weinert, Ed., Berlin, Germany: Springer-Verlag, 2009, pp. 64-70.</p>"},{"location":"fundamentals/operators/","title":"Operators","text":"<p>Physical, measurable quantities such as energy, position, and momentum are called observables and are represented by operators, which in QM are complex square matrices.</p> <p>These quantities correspond to properties of quantum systems, such as atoms or subatomic particles.</p> <p>Common operators</p> <p>Some common operators are the position operator \\(\\hat{x}\\), the momentum operator \\(\\hat{p}\\), and the energy operator \\(\\hat{H}\\). The energy operator \\(\\hat{H}\\) is called the Hamiltonian, and corresponds to the total energy of the given system.</p> <p>A physical quantity \\(\\mathcal{A}\\) is described by an operator \\(\\hat{A}\\), which acts on state vectors to produce another state vector:</p> \\[ \\hat{A}\\ket{\\psi} = \\ket{\\psi'}. \\] <p>Operators in QM are linear</p> <p>In QM, all operators are linear. Therefore, they distribute over superpositions just as you'd expect:</p> \\[ \\hat{A}\\Bigl(a\\ket{\\psi} + b\\ket{\\phi}\\Bigr) = a\\hat{A}\\ket{\\psi} + b\\hat{A}\\ket{\\phi} \\] <p>Operator addition</p> <p>Addition of operators is:</p> <ul> <li>associative: \\(\\hat{A} + (\\hat{B} + \\hat{C}) = (\\hat{A} + \\hat{B}) + \\hat{C}\\), and</li> <li>commutative: \\(\\hat{A} + \\hat{B} = \\hat{B} + \\hat{A}\\).</li> </ul> <p>Operator multiplication</p> <p>Multiplication of operators is defined through the way operators act on kets:</p> \\[ (\\hat{A}\\hat{B})\\ket{\\psi} = \\hat{A}\\bigl(\\hat{B}\\ket{\\psi}\\bigr) = \\hat{A}\\ket{\\psi'}, \\] <p>if \\(\\hat{B}\\ket{\\psi} = \\ket{\\psi'}\\).</p> <p>Multiplication of operators is:</p> <ul> <li>associative: \\(\\hat{A}(\\hat{B}\\hat{C}) = (\\hat{A}\\hat{B})\\hat{C}\\), but</li> <li>NOT necessarily commutative: \\(\\hat{A}\\hat{B} \\neq \\hat{B}\\hat{A}\\).</li> </ul>"},{"location":"fundamentals/operators/#matrix-elements","title":"Matrix elements","text":"<p>Let \\(\\hat{A}\\ket{\\phi} = \\ket{\\phi'}\\). If we take the inner product of a ket \\(\\ket{\\psi}\\) with \\(\\ket{\\phi'}\\), we end up with a structure that looks like the following:</p> \\[ \\braket{\\psi | \\phi'} = \\bra{\\psi} \\bigl(\\hat{A}\\ket{\\phi}\\bigr). \\] <p>Since the result of the inner product of two kets in general is a scalar \\(c \\in \\mathbb{C}\\), the above must of course also result in a scalar \\(c \\in \\mathbb{C}\\).</p> <p>The expression \\(\\bra{\\psi} \\bigl(\\hat{A}\\ket{\\phi}\\bigr)\\) is by definition equivalent to \\(\\bigl(\\bra{\\psi}\\hat{A}\\bigr)\\ket{\\phi}\\) \u2014 i.e. we can interpret it as either the operator \\(\\hat{A}\\) acting on the ket \\(\\ket{\\psi}\\), or as \\(\\hat{A}\\) acting on the bra \\(\\bra{\\phi}\\).</p> <p>Matrix element</p> <p>The following two expressions are equivalent:</p> \\[ \\bra{\\psi} \\bigl(\\hat{A}\\ket{\\phi}\\bigr) \\Leftrightarrow \\bigl(\\bra{\\psi}\\hat{A}\\bigr)\\ket{\\phi}. \\] <p>This structure is called a matrix element and is usually written without parentheses, as \\(\\bra{\\psi}\\hat{A}\\ket{\\phi}\\). Specifically, we would call this example the matrix element of the operator \\(\\hat{A}\\) with respect to \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\).</p> <p>Adjoint operator</p> <p>Let \\(\\ket{\\psi'} = \\hat{A}\\ket{\\psi}\\) be a ket in state space \\(\\mathcal{H}\\). Then, \\(\\bra{\\psi'} = \\bra{\\psi}\\hat{A}^\\dagger\\) is the corresponding bra in dual space \\(\\mathcal{H}^*\\).</p> <p>\\(\\hat{A}^\\dagger\\) is called the adjoint operator, and it is defined by taking the transpose of original operator and then taking the complex conjugate of each of its elements.</p> <p>Just like other operators in QM, the adjoint operator is linear. With \\(\\bra{\\psi} = a_1\\bra{\\psi_1} + a_2\\bra{\\psi_2}\\), we can distribute \\(\\hat{A}^\\dagger\\) to the left onto bras within parentheses like so:</p> \\[ \\bra{\\psi}\\hat{A}^\\dagger = \\bigl(a_1\\bra{\\psi_1} + a_2\\bra{\\psi_2}\\bigr)\\hat{A}^\\dagger = a_1\\bra{\\psi_1}\\hat{A}^\\dagger + a_2\\bra{\\psi_2}\\hat{A}^\\dagger \\]"},{"location":"fundamentals/operators/#hermitian-operators","title":"Hermitian operators","text":"<p>Observables = Hermitian operators</p> <p>In QM, all operators are observables \u2014 they are measurable properties of physical systems. In order for this to be true, the operators we deal with are all equal to their own adjoints:</p> \\[ \\hat{A} = \\hat{A}^\\dagger. \\] <p>An operator that obeys this property is called a Hermitian operator.</p> <p>Observables are represented by Hermitian operators because all Hermitian operators have real eigenvalues. Furthermore, Hermitian operators have orthogonal eigenstates, which we can normalize, letting us use them to form an orthonormal basis for the state space.</p> Proof: Hermitian operators have real eigenvalues <p>Let \\(\\hat{A}\\) be a Hermitian operator. The eigenvalue equation gives</p> \\[ \\begin{align}     \\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda} \\Leftrightarrow &amp;&amp; \\tiny{\\text{[multiply with $\\bra{\\lambda}$ from left]}}\\\\     \\bra{\\lambda}\\hat{A}\\ket{\\lambda} = \\lambda\\braket{\\lambda | \\lambda}. &amp;&amp;  \\end{align} \\] <p>The corresponding bra to \\(\\lambda\\ket{\\lambda}\\) is \\(\\lambda^*\\bra{\\lambda}\\), and the corresponding bra to \\(\\hat{A}\\ket{\\lambda}\\) is \\(\\bra{\\lambda}\\hat{A}^\\dagger\\). The eigenvalue equation \\(\\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda}\\) can thus be written as \\(\\bra{\\lambda}\\hat{A}^\\dagger = \\lambda^*\\bra{\\lambda}\\)</p> <p>Given that \\(\\hat{A}\\) is Hermitian, \\(\\hat{A} = \\hat{A}^\\dagger\\), we get:</p> \\[ \\begin{align}     \\bra{\\lambda}\\hat{A}\\ket{\\lambda} = \\lambda\\braket{\\lambda | \\lambda} \\Leftrightarrow &amp;&amp; \\\\     \\bra{\\lambda}\\hat{A}^\\dagger\\ket{\\lambda} = \\lambda\\braket{\\lambda | \\lambda} \\Leftrightarrow &amp;&amp; \\\\     \\lambda^*\\braket{\\lambda | \\lambda} = \\lambda\\braket{\\lambda | \\lambda} \\Rightarrow &amp;&amp; \\tiny{\\text{[from eigenvalue eq., bra version]}}\\\\     \\lambda^* = \\lambda. &amp;&amp; \\tiny{\\text{[assuming $\\ket{\\lambda} \\neq \\ket{\\text{null}}$]}}\\\\ \\end{align} \\] <p>There is only one way for a complex number \\(c \\in \\mathbb{C}\\) to be equal to its own complex conjugate: it cannot have an imaginary component, so it is real. QED.</p> Proof: Hermitian operators have eigenstates that form an orthonormal basis for \\(\\mathcal{H}\\) <p>Say we have an operator \\(\\hat{A}\\) with two different eigenvalues \\(\\lambda\\) and \\(\\mu\\) and their associated eigenvectors \\(\\ket{\\lambda}\\) and \\(\\ket{\\mu}\\) like so:</p> \\[ \\begin{align}     \\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda},\\tag{1}\\\\     \\hat{A}\\ket{\\mu} = \\mu\\ket{\\mu}\\tag{2}. \\end{align} \\] <p>We modify equation (1) to get:</p> \\[ \\begin{align}     \\hat{A}\\ket{\\lambda} = \\lambda\\ket{\\lambda} \\Leftrightarrow &amp;&amp; \\tiny{\\text{multiply with $\\bra{\\mu}$ from left}}\\\\     \\bra{\\mu}\\hat{A}\\ket{\\lambda} = \\lambda\\braket{\\mu | \\lambda}. &amp;&amp; \\end{align} \\] <p>We now modify equation (2) to get:</p> \\[ \\begin{align}     \\hat{A}\\ket{\\mu} = \\mu\\ket{\\mu} \\Leftrightarrow &amp;&amp; \\tiny{\\text{rewrite to dual space}}\\\\     \\bra{\\mu}\\hat{A}^\\dagger = \\mu^*\\bra{\\mu} \\Leftrightarrow &amp;&amp; \\tiny{\\text{$\\hat{A}$ is Hermitian and $\\mu$ is real}}\\\\     \\bra{\\mu}\\hat{A} = \\mu\\bra{\\mu} \\Leftrightarrow &amp;&amp; \\tiny{\\text{multiply with $\\ket{\\lambda}$ from right}}\\\\     \\bra{\\mu}\\hat{A}\\ket{\\lambda} = \\mu\\braket{\\mu | \\lambda}. &amp;&amp; \\end{align} \\] <p>Since the left-hand sides of both equations are equal, we can equate the right-hand sides to get:</p> \\[ \\begin{align}     \\lambda\\braket{\\mu | \\lambda} = \\mu\\braket{\\mu | \\lambda} \\Leftrightarrow &amp;&amp; \\\\     \\lambda\\braket{\\mu | \\lambda} - \\mu\\braket{\\mu | \\lambda} = 0 \\Leftrightarrow &amp;&amp; \\\\     \\braket{\\mu | \\lambda}(\\lambda - \\mu) = 0. \\end{align} \\] <p>Since \\(\\lambda \\neq \\mu\\), the only way this can be true is if \\(\\braket{\\mu | \\lambda} = 0\\), which means that \\(\\ket{\\lambda}\\) and \\(\\ket{\\mu}\\) are orthogonal. QED.</p>"},{"location":"fundamentals/operators/#compatibility-of-observables","title":"Compatibility of observables","text":"<p>Commutator</p> <p>We define the commutator \\([\\hat{A}, \\hat{B}]\\) of two operators \\(\\hat{A}\\) and \\(\\hat{B}\\) as:</p> \\[ [\\hat{A}, \\hat{B}] = \\hat{A}\\hat{B} - \\hat{B}\\hat{A}. \\] <p>We say that two observables \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) are compatible if the corresponding operators \\(\\hat{A}\\) and \\(\\hat{B}\\) commute.</p> <p>Compatible operators</p> <p>Two operators \\(\\hat{A}\\) and \\(\\hat{B}\\) are compatible if they commute:</p> \\[ \\hat{A}\\hat{B} = \\hat{B}\\hat{A}. \\] <p>We can express this in terms of the commutator \\([\\hat{A}, \\hat{B}]\\) as:</p> \\[ [\\hat{A}, \\hat{B}] = 0. \\] <p>If two observables are compatible, it means that they can be measured simultaneously.</p>"},{"location":"fundamentals/operators/#unitary-operators","title":"Unitary operators","text":"<p>Unitary operators</p> <p>Another important subset of operators are the unitary operators. An operator is unitary if its inverse is equal to its adjoint:</p> \\[ \\hat{A}^{-1} = \\hat{A}^\\dagger. \\]"},{"location":"fundamentals/outer-product/","title":"Outer product","text":"<p>We've previously talked about the inner product, introducing the notion of a bracket \\(\\braket{\\psi | \\phi}\\).</p> <p>So is it possible to have a \"ket-bra\"? The answer is yes, and it's written like \\(\\ket{\\phi}\\bra{\\psi}\\)(1).</p> <ol> <li>In handwriting, we usually write the left and right angle brackets without the space in the middle, a bit like a cross: \\(|\\phi \\times \\psi |\\). To avoid confusion though, I will stick to \\(\\ket{\\phi}\\bra{\\psi}\\).</li> </ol> <p>The outer product \\(\\ket{\\phi}\\bra{\\psi}\\) is another way of expressing an operator, as the result of executing the outer product results in an operator.</p> <p>Inner vs. outer product, result type</p> <p>Let \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\) be two state vectors in \\(\\mathcal{H}\\).</p> <ul> <li>The inner product \\(\\braket{\\psi | \\phi} = a \\in \\mathbb{C}\\) results in a complex number.</li> <li>The outer product \\(\\ket{\\phi} \\bra{\\psi} = \\hat{A} \\in \\mathbb{C}^{n \\times n}\\) results in an operator, which can be written as a matrix.</li> </ul>"},{"location":"fundamentals/state-vectors-hilbert-space/","title":"State vectors and Hilbert space","text":"<p>The state of a closed physical system can be described by a state vector or wave function which is an element of some complex Hilbert space, denoted here with \\(\\mathcal{H}\\).</p> <p>Throughout these notes and in most other QM resources available, state vectors are written in Dirac notation and are then called kets. A ket is written with a vertical bar (<code>|</code>), followed by the name or label of the ket (often a Greek letter like \\(\\psi\\)(1) or \\(\\phi\\)(2)), followed by a right-angle bracket (<code>&gt;</code>). The following is a ket: \\(\\ket{\\psi}\\).</p> <ol> <li>pronounced \"sy\", \"psy\", \"see\", or \"psee\", depending on who you ask</li> <li>pronounced \"fy\" or \"fee\", depending on who you ask</li> </ol> <p>\\(\\mathcal{H}\\) can be infinite-dimensional. If we think of vectors as an ordered list of numbers, this means that the list can be infinitely long. An equivalent way to think about it is that we might need an infinite number of basis vectors to represent a given vector in \\(\\mathcal{H}\\).</p> <p>Another important thing to remember about \\(\\mathcal{H}\\) is that whenever we talk about scalars, we mean complex scalars. In other words, vector components are complex, and the scalars we scale vectors with are complex, and so on. Always assume that the numbers we work with are complex, i.e. can include an imaginary term, unless otherwise specified.</p> <p>Just like with vectors in Euclidean space, we can scale state vectors by multiplying them with scalars. We can also add state vectors together, and the result will be another state vector.</p> <p>If a state vector is written as a linear combination of other state vectors, we say that the former is a superposition of the latter. For example, if we have a state \\(\\ket{\\psi} = a\\ket{\\psi_1} + b\\ket{\\psi_2}\\), we would say that \\(\\ket{\\psi}\\) is a superposition of \\(\\ket{\\psi_1}\\) and \\(\\ket{\\psi_2}\\).</p> <p>State vector? Ket? Wave function?</p> <p>What is really the difference between a state vector, a ket, and a wave function?</p> <p>A state vector is a vector in Hilbert space that is associated with a physical state that the system can be in(1).</p> <ol> <li>Note that this does not mean that the state of the system necessarily corresponds to that exact vector. In other words, a state vector \\(\\bar{v}\\) corresponds to one specific physical state \\(V\\) (\\(\\bar{v} \\rightarrow V\\)), but a physical state does not necessarily correspond to that same vector (\\(V \\nrightarrow \\bar{v})\\). More on this later.</li> </ol> <p>A ket is a state vector in Dirac notation.</p> <p>A wave function is a complex-valued function that is a mathematical description of a quantum system. It is a different way of representing the state of a quantum system. It is less general than that of the vector representation, as it implicitly chooses a basis.</p>"},{"location":"fundamentals/state-vectors-hilbert-space/#references","title":"References","text":"<p>[1] Professor M does Science, Cambridge, U.K. Dirac Notation: State Space And Dual Space. (May 23, 2020). Accessed: Jul. 3, 2024. [Online Video]. Available: https://www.youtube.com/watch?v=hJoWM9jf0gU.</p>"}]}