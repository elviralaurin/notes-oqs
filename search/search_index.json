{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>These notes are my attempt to understand quantum mechanics and open quantum systems. They are written in an effort to be easily understood rather than condensed into a clean set of definitions and proofs. As a result, they tend to get a bit wordy, so be warned.</p> <p>All mistakes are my own. If you spot an error or disagree with anything, please leave a comment or a pull request in the GitHub repository for this project.</p>"},{"location":"abstract-vector-spaces/","title":"Abstract vector spaces","text":"<p>Why is this relevant?</p> <p>The language of quantum mechanics is linear algebra, and everything we do is done in special vector spaces called Hilbert spaces. Hilbert spaces can be infinite-dimensional (don't worry, we'll get there), and the \"vectors\" we work with aren't really vectors in the sense you'd learn about in a basic linear algebra course. Instead, they are functions.</p> <p>Abstract linear algebra is what happens when we do normal linear algebra, but we stop caring about the details of what a vector \"actually\" is. Instead, we just formulate a set of definitions (the same ones as in normal linear algebra, in fact), and base all our logic on those definitions. In other words, we take the findings of normal linear algebra and abstract them so that we are able to apply them to a wider range of objects and topics.</p> <p>For example, consider the vector space \\(R^3\\). It contains any vector in any direction you can literally think of. You can add any two (or more) vectors, stretch or shrink them by multiplying them with scalars, all to your heart's content. Also, you can condense the information of a vector space into just a set of basis vectors, which you can then use as a kind of common language to express any vector in the space. The basis vectors can be arbitrarily chosen, as long as there is at least one representative of each direction of the space, which in math is called linear independence.</p> <p>If we play around with vectors like this for a while, we will start noticing certain properties that any vector space has to have in order to actually be a vector space. Formally, there are eight such properties, and we'll use them as axioms for defining what a vector space actually is.<sup>1</sup></p> <p>Vector spaces</p> <p>A vector space is a set \\(V\\), that has an addition operation (\\(+\\)) and a scalar multiplication operation (\\(\\cdot\\)), for which the following eight axioms hold (assume that all symbols with an arrow (e.g. \\(\\vec{v}\\)) are elements of \\(V\\), and all lowercase symbols without an arrow (e.g. \\(a\\)), are scalars):</p> <ol> <li>When adding elements, the order of the arguments doesn't matter: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\)</li> <li>When adding more than two arguments, it doesn't matter which two arguments we add together first: \\((\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\\)</li> <li>There is a special element \\(0 \\in V\\) that, when added to any other element \\(\\vec{v} \\in V\\), leaves \\(\\vec{v}\\) unchanged: \\(\\vec{v} + 0 = \\vec{v}\\)</li> <li>Every single elements \\(\\vec{v} \\in V\\) has an opposite \\(-\\vec{v} \\in V\\), so that when you add them together, you get \\(0\\): \\(\\vec{v} + (-\\vec{v}) = 0\\)</li> <li>When multiplying \\(1\\) with any element \\(\\vec{v}\\), that element remains unchanged: \\(1 \\cdot \\vec{v} = \\vec{v}\\)</li> <li>There is a distributive rule for sums of scalars like so: \\((a + b)\\cdot\\vec{v} = a \\cdot \\vec{v} + b \\cdot \\vec{v}\\)</li> <li>When multiplying scalars and elements of \\(V\\), the order doesn't matter: \\((a \\cdot b) \\cdot \\vec{v} = a \\cdot (b \\cdot \\vec{v})\\)</li> <li>There is a distributive rule for sums of vectors like so: \\(a \\cdot (\\vec{v} + \\vec{u}) = a \\cdot \\vec{v} + a \\cdot \\vec{u}\\)</li> </ol> <p>I'm playing a little fast and loose with the term \"scalar\" here. At this point, just assume that a scalar is a real number.</p> <p>All of these properties probably feel quite trivial. There are some interesting things to note though. First, the way we've written it, whenever we use scalar multiplication we always write the scalar on the left and the element of \\(V\\) on the right.</p> <p>Now, let's consider the set of all polynomials \\(\\mathcal{P}(x)\\).</p> <p>Say we have two polynomials \\(p_1(x)\\) and \\(p_2(x)\\). No matter what \\(p_1(x)\\) and \\(p_2(x)\\) actually are, the result will always be another polynomial, with the same degree as whichever of \\(p_1(x)\\) or \\(p_2(x)\\) had the highest degree. We can multiply a polynomial by a scalar.</p>"},{"location":"abstract-vector-spaces/#references","title":"References","text":"<ol> <li> <p>T. Smits. (2023). Abstract Linear Algebra [Online]. Available: https://www.math.ucla.edu/~tsmits/115coursenotes.pdf \u21a9</p> </li> </ol>"},{"location":"hilbert-spaces/","title":"Hilbert spaces","text":"<p>\"A Hilbert space is a vector space equipped with an inner product such that the norm, which is defined as \\(|f| = \\sqrt{f \\cdot f}\\), turns the space into a complete metric space\"</p>"}]}