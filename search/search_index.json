{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>These notes are my attempt to understand quantum mechanics and open quantum systems. They are written in an effort to be easily understood rather than condensed into a clean set of definitions and proofs. As a result, they tend to get a bit wordy, so be warned.</p> <p>All mistakes are my own. If you spot an error or disagree with anything, please leave a comment or a pull request in the GitHub repository for this project.</p>"},{"location":"abstract-vector-spaces/","title":"Abstract vector spaces","text":"<p>Why is this relevant?</p> <p>The language of quantum mechanics is linear algebra, and everything we do is done in special vector spaces called Hilbert spaces. Hilbert spaces can be infinite-dimensional (don't worry, we'll get there), and the \"vectors\" we work with aren't really vectors in the sense you'd learn about in a basic linear algebra course. Instead, they are functions.</p>"},{"location":"abstract-vector-spaces/#what-we-mean-by-abstract","title":"What we mean by \"abstract\"","text":"<p>Abstract linear algebra is what happens when we do normal linear algebra, but we stop caring about the details of what a vector \"actually\" is. Instead, we just formulate a set of definitions (the same ones as in normal linear algebra, in fact), and base all our logic on those definitions - we make no additional assumptions about the underlying mathematical objects. In other words, we take the findings of normal linear algebra and abstract them so that we are able to apply them to a wider range of objects and topics.</p> <p>For example, consider the vector space \\(R^3\\). It contains any vector in any direction you can literally think of. You can add any two (or more) vectors, stretch or shrink them by multiplying them with scalars, all to your heart's content. Also, you can condense the information of a vector space into just a set of basis vectors, which you can then use as a kind of common language to express any vector in the space. The basis vectors can be arbitrarily chosen, as long as there is at least one representative of each direction of the space, which in math is called linear independence.</p> <p>If we play around with vectors like this for a while, we will start noticing certain properties that seem to be universal for all vector spaces. Formally, there are eight such properties, and we'll use them as axioms for defining what a vector space actually is.</p>"},{"location":"abstract-vector-spaces/#what-makes-a-vector-space","title":"What makes a vector space?","text":"<p>Vector spaces</p> <p>A vector space is a set \\(V\\), that has an addition operation (\\(+\\)) and a scalar multiplication operation (\\(\\cdot\\)), for which the following eight axioms hold (assume that all symbols with an arrow \u2014 e.g. \\(\\vec{v}\\) \u2014 are elements of the set \\(V\\), and all (lowercase) symbols without an arrow \u2014 e.g. \\(a\\) \u2014 are scalars)[1]:</p> <ol> <li>When adding elements, the order of the arguments doesn't matter: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\)</li> <li>When adding more than two arguments, it doesn't matter which two arguments we add together first: \\((\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\\)</li> <li>There is a special element \\(\\vec{0} \\in V\\) that, when added to any other element \\(\\vec{v}\\), leaves \\(\\vec{v}\\) unchanged: \\(\\vec{v} + \\vec{0} = \\vec{v}\\)</li> <li>Every single element \\(\\vec{v}\\) has an opposite \\(-\\vec{v} \\in V\\), so that when you add them together, you get \\(\\vec{0}\\): \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\)</li> <li>When multiplying the scalar \\(1\\) with any element \\(\\vec{v}\\), that element remains unchanged: \\(1 \\cdot \\vec{v} = \\vec{v}\\)</li> <li>There is a distributive rule for sums of scalars like so: \\((a + b)\\cdot\\vec{v} = a \\cdot \\vec{v} + b \\cdot \\vec{v}\\)</li> <li>When multiplying multiple scalars with elements of \\(V\\), the order doesn't matter: \\((a \\cdot b) \\cdot \\vec{v} = a \\cdot (b \\cdot \\vec{v})\\)</li> <li>There is a distributive rule for sums of elements like so: \\(a \\cdot (\\vec{v} + \\vec{u}) = a \\cdot \\vec{v} + a \\cdot \\vec{u}\\)</li> </ol> <p>I'm playing a little fast and loose with the term \"scalar\" here. At this point, just assume that a scalar is a real number.</p> <p>All of these properties probably feel quite trivial. There are some interesting things to note though. First, the way we've written it, whenever we use scalar multiplication we always write the scalar on the left and the element of \\(V\\) on the right. Second \u2014 and most important \u2014 we're really not making any assumptions about the elements of \\(V\\) at all. What this list of properties is really telling us are expectations about how the operations of addition and scalar multiplication behave. So what makes a vector space a vector space really isn't the vectors themselves; it's what we can do with them.</p>"},{"location":"abstract-vector-spaces/#thats-a-vector-space","title":"That's a vector space?","text":"<p>Let's consider the set of all polynomials of degree \\(n \\leq 2\\). We'll denote this set as \\(\\mathcal{P}^2\\).</p> <p>Let's grab three elements of \\(\\mathcal{P}^2\\); call them \\(p_1(x)\\), \\(p_2(x)\\), and \\(p_3(x)\\).</p> <p>Is \\(\\mathcal{P}^2\\) a vector space? Let's check:</p> <ol> <li>Does the order matter when adding two elements, like \\(p_1(x) + p_2(x)\\)? No, so check!</li> <li>If we add more than two elements together, does the order in which we do things matter? Nope, so check!</li> <li>Is there a weird element that leaves other elements unchanged when adding it to them? With all coefficients equal to 0, we just get zero, so zero is technically a polynomial in the set, so check!</li> <li>Does every polynomial have an opposite, so that if we add it with its opposite we get zero? If we take a polynomial, copy it, and reverse the signs of all coefficients of the copy, then we get exactly such an opposite. So check!</li> <li>If we multiply any polynomial with the scalar 1, we get that same polynomial back. So check!</li> <li>Does the distributive rule for sums of scalars work as expected? \\((a + b)p_1(x) = ap_1(x) + bp_1(x)\\), so yup, and check!</li> <li>Does the order matter when multiplying with more than one scalar? \\((ab)p_1(x) = abp_1(x) = a(bp_1(x))\\) \u2014 it does not, so check!</li> <li>Does the distributive rule for sums of elements work as expected? \\(a(p_1(x) + p_2(x)) = ap_1(x) + ap_2(x)\\), so yes it does \u2014 check!</li> </ol> <p>Why, look at that! It seems that our set is a vector space.</p> <p>\"Okay,\" you say. \"So what?\"</p> <p>Well, now that we know that the set of these polynomials really is just a vector space in a fancy outfit, we can apply all we know about \"normal\" vector spaces to it. We would probably not normally think of polynomials as vectors, but we have just shown that we just might as well look at them through that lense. And maybe we'll spot something interesting when we do.</p>"},{"location":"abstract-vector-spaces/#references","title":"References","text":"<p>[1] T. Smits. (2023). Abstract Linear Algebra [Online]. Available: https://www.math.ucla.edu/~tsmits/115coursenotes.pdf</p>"},{"location":"quantum-state-space/","title":"Quantum state space","text":"<p>The mathematics of quantum mechanics is done in a special type of vector space we'll call quantum state space, or just state space [1].</p> <p>The details of the state space will differ depending on the situation. Not only that, but different people also seem to have quite different opinions on exactly what properties the state space has to have (or at least, which of those properties it is relevant to even mention).</p> <p>In these notes, we will denote the state space with \\(\\mathcal{H}\\). What we generally mean when we refer to \\(\\mathcal{H}\\) here is a complete, separable, complex, possibly infinite-dimensional Hilbert space(1).</p> <ol> <li>For details on what all these terms mean, see chapter 2 of [2].</li> </ol> <p>If you're like me, that is a pretty hefty list of mathematical jargon, but the only word that really is relevant right now is complex. From now on, whenever we talk about numbers, we mean complex numbers, unless otherwise specified. That includes all scalars, vector components, etc.</p>"},{"location":"quantum-state-space/#vectors-in-state-space","title":"Vectors in state space","text":"<p>As mentioned in the section about abstract vector spaces, the vectors of \\(\\mathcal{H}\\) aren't the familiar list-of-numbers we learn about in our basic linear algebra courses. But, we'll get the practicalities out of the way first.</p> <p>Kets</p> <p>The vectors of \\(\\mathcal{H}\\) are called kets, and are written using a pipe symbol (<code>|</code>), followed by the label of the ket, followed by a right angle bracket (<code>&gt;</code>) [1]:</p> \\[ \\ket{\\psi} \\in \\mathcal{H}. \\] <p>We will usually label our kets with Greek letters, such as with \\(\\psi\\) (\"psi\") or \\(\\phi\\) (\"phi\"), but no matter what, the labels are just that \u2014 labels. You can name them \\(\\ket{\\text{Erwin}}\\) or \\(\\ket{\\text{Werner}}\\) for all anyone cares.</p> <p>Quantum states \\(\\neq\\) elements of \\(\\mathcal{H}\\)</p> <p>Even though we call \\(\\mathcal{H}\\) \"quantum state space\", the elements of \\(\\mathcal{H}\\) aren't quantum states, for reasons we'll get to later. For now we'll just say that the elements of state space are called kets, and simply ignore the physical interpretation \u2014 we'll get to that.</p>"},{"location":"quantum-state-space/#inner-products","title":"Inner products","text":"<p>Just like we can take the dot product of two vectors in \"normal\"(1) vector spaces, there is a corresponding operation in state space.</p> <ol> <li>From now on, I'll stop writing \"normal\" and start writing what I actually mean: Euclidean vector spaces.</li> </ol> <p>The operation is called inner product, and isn't specific to quantum mechanics. In linear algebra, the inner product of two vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) of some vector space \\(V\\) is usually written as \\((\\vec{u}, \\vec{v})\\).</p> <p>In quantum mechanics, we use a special notation called Dirac notation or Bra-Ket notation(1).</p> <ol> <li>\"Bra-Ket\" from the word \"bracket\" \u2014 we have already met the kets, and will talk about the bras in a little bit.</li> </ol> <p>Inner product</p> <p>The inner product of two kets \\(\\ket{\\psi}, \\ket{\\phi} \\in \\mathcal{H}\\) is written as:</p> \\[ (\\ket{\\psi}, \\ket{\\phi}) := \\braket{\\psi|\\phi} \\] <p>The inner product always results in a non-negative real number.</p> <p>This might look strange at first \u2014 I know it did to me \u2014 but there are some neat algebraic manipulation tricks that Bra-Ket notation allows us to use. It took some time for me to get used to it though, so I'll go slow in introducing it here.</p> <p>Notice that when taking the inner product of two kets \\(\\ket{\\psi}\\) and \\(\\ket{\\phi}\\), we can look at it just in terms of the symbols as flipping the first ket (like \\(\\bra{\\psi}\\)), and smushing it together with the second ket (like \\(\\bra{\\psi}\\ket{\\phi}\\)), and finally removing one of the duplicate vertical bars in the middle (arriving at the notation for the inner product from above; \\(\\braket{\\psi|\\phi}\\)).</p> <p>That flipped ket, \\(\\bra{\\phi}\\), has a name \u2014 it's a bra, the second part of the \"bracket\" of Bra-Ket notation \u2014 and we'll talk a little bit more about it later.</p>"},{"location":"quantum-state-space/#norm","title":"Norm","text":"<p>Just as in Euclidean(1) vector spaces, the inner product raises the following question: can we take the inner product of a vector with itself, and what does that mean?</p> <ol> <li>See for example Vectors in Euclidean Space on LibreTexts Mathematics</li> </ol> <p>In Euclidean vector spaces, taking the inner (dot) product of a vector with itself results in a real number called the norm of the vector, which corresponds to its length or magnitude.</p> <p>For vectors in \\(\\mathcal{H}\\), we don't have the luxury of such simple interpretations(1), but we can nevertheless still calculate the norm of kets.</p> <ol> <li>There is still a real, physical interpretation, but as it's slightly more confusing than just \"length\", we'll leave it at this for now.</li> </ol> <p>Norm of a ket</p> <p>The norm of a ket \\(\\ket{\\psi} \\in \\mathcal{H}\\) is defined as the result of taking the inner product of \\(\\ket{\\psi}\\) with itself:</p> \\[ \\braket{\\psi|\\psi} \\] <p>Of course, since the inner product always results in a non-negative real number, the norm is a non-negative real number.</p>"},{"location":"quantum-state-space/#references","title":"References","text":"<p>[1] Professor M does Science, Cambridge, U.K. Dirac Notation: State Space And Dual Space. (May 23, 2020). Accessed: Jul. 3, 2024. [Online Video]. Available: https://www.youtube.com/watch?v=hJoWM9jf0gU [2] J. von Neumann, Mathematical Foundations of Quantum Mechanics, N. A. Wheeler, Ed., 2018 ed. Princeton, NJ, USA: Princeton Univ. Press, 2018, doi: 10.1017/9781316995433.  </p>"}]}