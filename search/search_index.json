{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>These notes are my attempt to understand quantum mechanics and open quantum systems. They are written in an effort to be easily understood rather than condensed into a clean set of definitions and proofs. As a result, they tend to get a bit wordy, so be warned.</p> <p>All mistakes are my own. If you spot an error or disagree with anything, please leave a comment or a pull request in the GitHub repository for this project.</p>"},{"location":"abstract-vector-spaces/","title":"Abstract vector spaces","text":"<p>Why is this relevant?</p> <p>The language of quantum mechanics is linear algebra, and everything we do is done in special vector spaces called Hilbert spaces. Hilbert spaces can be infinite-dimensional (don't worry, we'll get there), and the \"vectors\" we work with aren't really vectors in the sense you'd learn about in a basic linear algebra course. Instead, they are functions.</p>"},{"location":"abstract-vector-spaces/#what-we-mean-by-abstract","title":"What we mean by \"abstract\"","text":"<p>Abstract linear algebra is what happens when we do normal linear algebra, but we stop caring about the details of what a vector \"actually\" is. Instead, we just formulate a set of definitions (the same ones as in normal linear algebra, in fact), and base all our logic on those definitions - we make no additional assumptions about the underlying mathematical objects. In other words, we take the findings of normal linear algebra and abstract them so that we are able to apply them to a wider range of objects and topics.</p> <p>For example, consider the vector space \\(R^3\\). It contains any vector in any direction you can literally think of. You can add any two (or more) vectors, stretch or shrink them by multiplying them with scalars, all to your heart's content. Also, you can condense the information of a vector space into just a set of basis vectors, which you can then use as a kind of common language to express any vector in the space. The basis vectors can be arbitrarily chosen, as long as there is at least one representative of each direction of the space, which in math is called linear independence.</p> <p>If we play around with vectors like this for a while, we will start noticing certain properties that seem to be universal for all vector spaces. Formally, there are eight such properties, and we'll use them as axioms for defining what a vector space actually is.</p>"},{"location":"abstract-vector-spaces/#what-makes-a-vector-space","title":"What makes a vector space?","text":"<p>Vector spaces</p> <p>A vector space is a set \\(V\\), that has an addition operation (\\(+\\)) and a scalar multiplication operation (\\(\\cdot\\)), for which the following eight axioms hold (assume that all symbols with an arrow \u2014 e.g. \\(\\vec{v}\\) \u2014 are elements of the set \\(V\\), and all (lowercase) symbols without an arrow \u2014 e.g. \\(a\\) \u2014 are scalars)[1]:</p> <ol> <li>When adding elements, the order of the arguments doesn't matter: \\(\\vec{v} + \\vec{u} = \\vec{u} + \\vec{v}\\)</li> <li>When adding more than two arguments, it doesn't matter which two arguments we add together first: \\((\\vec{u} + \\vec{v}) + \\vec{w} = \\vec{u} + (\\vec{v} + \\vec{w})\\)</li> <li>There is a special element \\(\\vec{0} \\in V\\) that, when added to any other element \\(\\vec{v}\\), leaves \\(\\vec{v}\\) unchanged: \\(\\vec{v} + \\vec{0} = \\vec{v}\\)</li> <li>Every single element \\(\\vec{v}\\) has an opposite \\(-\\vec{v} \\in V\\), so that when you add them together, you get \\(\\vec{0}\\): \\(\\vec{v} + (-\\vec{v}) = \\vec{0}\\)</li> <li>When multiplying the scalar \\(1\\) with any element \\(\\vec{v}\\), that element remains unchanged: \\(1 \\cdot \\vec{v} = \\vec{v}\\)</li> <li>There is a distributive rule for sums of scalars like so: \\((a + b)\\cdot\\vec{v} = a \\cdot \\vec{v} + b \\cdot \\vec{v}\\)</li> <li>When multiplying multiple scalars with elements of \\(V\\), the order doesn't matter: \\((a \\cdot b) \\cdot \\vec{v} = a \\cdot (b \\cdot \\vec{v})\\)</li> <li>There is a distributive rule for sums of elements like so: \\(a \\cdot (\\vec{v} + \\vec{u}) = a \\cdot \\vec{v} + a \\cdot \\vec{u}\\)</li> </ol> <p>I'm playing a little fast and loose with the term \"scalar\" here. At this point, just assume that a scalar is a real number.</p> <p>All of these properties probably feel quite trivial. There are some interesting things to note though. First, the way we've written it, whenever we use scalar multiplication we always write the scalar on the left and the element of \\(V\\) on the right. Second \u2014 and most important \u2014 we're really not making any assumptions about the elements of \\(V\\) at all. What this list of properties is really telling us are expectations about how the operations of addition and scalar multiplication behave. So what makes a vector space a vector space really isn't the vectors themselves; it's what we can do with them.</p>"},{"location":"abstract-vector-spaces/#thats-a-vector-space","title":"That's a vector space?","text":"<p>Let's consider the set of all polynomials of degree \\(n \\leq 2\\). We'll denote this set as \\(\\mathcal{P}^2\\).</p> <p>Let's grab three elements of \\(\\mathcal{P}^2\\); call them \\(p_1(x)\\), \\(p_2(x)\\), and \\(p_3(x)\\).</p> <p>Is \\(\\mathcal{P}^2\\) a vector space? Let's check:</p> <ol> <li>Does the order matter when adding two elements, like \\(p_1(x) + p_2(x)\\)? No, so check!</li> <li>If we add more than two elements together, does the order in which we do things matter? Nope, so check!</li> <li>Is there a weird element that leaves other elements unchanged when adding it to them? With all coefficients equal to 0, we just get zero, so zero is technically a polynomial in the set, so check!</li> <li>Does every polynomial have an opposite, so that if we add it with its opposite we get zero? If we take a polynomial, copy it, and reverse the signs of all coefficients of the copy, then we get exactly such an opposite. So check!</li> <li>If we multiply any polynomial with the scalar 1, we get that same polynomial back. So check!</li> <li>Does the distributive rule for sums of scalars work as expected? \\((a + b)p_1(x) = ap_1(x) + bp_1(x)\\), so yup, and check!</li> <li>Does the order matter when multiplying with more than one scalar? \\((ab)p_1(x) = abp_1(x) = a(bp_1(x))\\) \u2014 it does not, so check!</li> <li>Does the distributive rule for sums of elements work as expected? \\(a(p_1(x) + p_2(x)) = ap_1(x) + ap_2(x)\\), so yes it does \u2014 check!</li> </ol> <p>Why, look at that! It seems that our set is a vector space.</p> <p>\"Okay,\" you say. \"So what?\"</p> <p>Well, now that we know that the set of these polynomials really is just a vector space in a fancy outfit, we can apply all we know about \"normal\" vector spaces to it. We would probably not normally think of polynomials as vectors, but we have just shown that we just might as well look at them through that lense. And maybe we'll spot something interesting when we do.</p>"},{"location":"abstract-vector-spaces/#references","title":"References","text":"<p>[1] T. Smits. (2023). Abstract Linear Algebra [Online]. Available: https://www.math.ucla.edu/~tsmits/115coursenotes.pdf</p>"},{"location":"quantum-state-space/","title":"Quantum state space","text":"<p>The mathematics of quantum mechanics is done in a special type of vector space we'll call quantum state space, or just state space.</p> <p>Formally, the state space will be different depending on the situation. As such, there really is no \"the\" state space, just the one we happen to be talking about at the moment. We will denote the state space at hand by \\(\\mathcal{H}\\). Just remember that what \\(\\mathcal{H}\\) really is can be quite different depending on who you ask, and during what circumstances.</p> <p>In these notes, we will try to be consistent in the use of \\(\\mathcal{H}\\). What we generally refer to here as a state space \\(\\mathcal{H}\\) is a complete, separable, complex, possibly infinite-dimensional Hilbert space(1).</p> <ol> <li>For details on what all these terms mean, see chapter 2 of [1].</li> </ol> <p>If you're like me, that is a pretty hefty list of mathematical jargon, but the only word that is really relevant at this moment is complex. From now on, whenever we talk about numbers, we mean complex numbers, unless otherwise specified. That includes all scalars, vector components, etc.</p>"},{"location":"quantum-state-space/#vectors-in-state-space","title":"Vectors in state space","text":"<p>As mentioned in the section about abstract vector spaces, the vectors of \\(\\mathcal{H}\\) aren't the familiar list-of-numbers we learn about in our basic linear algebra courses. But, we'll get the practicalities out of the way first.</p> <p>Kets</p> <p>The vectors of \\(\\mathcal{H}\\) are called kets, and are written using a pipe symbol (<code>|</code>), followed by the label of the ket, followed by a right angle bracket (<code>&gt;</code>):</p> \\[ \\ket{\\psi} \\in \\mathcal{H}. \\] <p>We will usually label our kets with Greek letters, such as with \\(\\psi\\) (\"psi\") or \\(\\phi\\) (\"phi\"), but no matter what, the labels are just that \u2014 labels. You can name them \\(\\ket{\\text{Erwin}}\\) or \\(\\ket{\\text{Werner}}\\) for all anyone cares.</p> <p>Quantum states \\(\\neq\\) elements of \\(\\mathcal{H}\\)</p> <p>Even though we call \\(\\mathcal{H}\\) \"quantum state space\", the elements of \\(\\mathcal{H}\\) aren't quantum states, for reasons we'll get to later. For now we'll just say that the elements of state space are called kets, and get into the physical interpretation later.</p>"},{"location":"quantum-state-space/#inner-products","title":"Inner products","text":"<p>Just like we can take the dot product of two vectors in \"normal\" vector spaces, there is a corresponding operation in state space.</p> <p>The operation is called inner product, and isn't specific for quantum mechanics. In linear algebra, the inner product of two vectors \\(\\vec{u}\\) and \\(\\vec{v}\\) of some vector space \\(V\\) is usually written as \\((\\vec{u}, \\vec{v})\\).</p> <p>In quantum mechanics, we use a special notation called Dirac notation or Bra-Ket notation(1).</p> <ol> <li>\"Bra-Ket\" from the word \"bracket\" \u2014 we have already met the kets, and will talk about the bras in a little bit.</li> </ol> <p>Inner product</p> <p>The inner product of two kets \\(\\ket{\\psi}, \\ket{\\phi} \\in \\mathcal{H}\\) is written as:</p> \\[ (\\ket{\\psi}, \\ket{\\phi}) := \\braket{\\psi|\\phi} \\]"},{"location":"quantum-state-space/#norm","title":"Norm","text":""},{"location":"quantum-state-space/#references","title":"References","text":"<p>[1] J. von Neumann, Mathematical Foundations of Quantum Mechanics, N. A. Wheeler, Ed., 2018 ed. Princeton, NJ, USA: Princeton Univ. Press, 2018, doi: 10.1017/9781316995433.</p>"}]}